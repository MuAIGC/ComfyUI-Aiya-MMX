# ~/ComfyUI/custom_nodes/ComfyUI-Aiya-MMX/nodes/openai_API.py
from __future__ import annotations
import io
import json
import base64
import time
import uuid
import threading
import requests
import torch
from PIL import Image
from ..register import register_node
from ..mmx_utils import pil2tensor, tensor2pil

# ---------- å…¨å±€å¹¶å‘ç¼“å­˜ ----------
_result_cache = {}
_processing_events = {}  # task_id -> threading.Event()
_cache_lock = threading.Lock()
_CACHE_TTL = 600  # 10åˆ†é’Ÿè¿‡æœŸ

def _cleanup_cache():
    now = time.time()
    expired = [k for k, (ts, _) in _result_cache.items() if now - ts > _CACHE_TTL]
    for k in expired:
        del _result_cache[k]

def cache_result(task_id: str, tensor: torch.Tensor | None):
    """å­˜å…¥ç»“æœå¹¶é€šçŸ¥ç­‰å¾…è€…"""
    with _cache_lock:
        _cleanup_cache()
        _result_cache[task_id] = (time.time(), tensor)
        if task_id in _processing_events:
            _processing_events[task_id].set()

def get_result(task_id: str) -> torch.Tensor | None:
    """è·å–ç»“æœï¼ˆéé˜»å¡ï¼‰"""
    if not task_id:
        return None
    with _cache_lock:
        if task_id in _result_cache:
            ts, tensor = _result_cache[task_id]
            if time.time() - ts < _CACHE_TTL:
                return tensor
            else:
                del _result_cache[task_id]
        return None

def wait_for_result(task_id: str, timeout: float = 300) -> torch.Tensor | None:
    """é˜»å¡ç­‰å¾…ç»“æœ"""
    if not task_id:
        return None
    
    # å…ˆæ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
    result = get_result(task_id)
    if result is not None:
        return result
    
    # ç­‰å¾… Event
    event = None
    with _cache_lock:
        if task_id in _processing_events:
            event = _processing_events[task_id]
    
    if event:
        event.wait(timeout)
        return get_result(task_id)
    return None


# ---------- é€šç”¨å·¥å…· ----------
def tensor2pil_single(t: torch.Tensor) -> Image.Image:
    if t.dim() == 4:
        t = t.squeeze(0)
    t = (t.clamp(0, 1) * 255).byte().cpu()
    return Image.fromarray(t.numpy())

def decode_b64_to_tensor(b64_str: str):
    img_bytes = base64.b64decode(b64_str)
    pil = Image.open(io.BytesIO(img_bytes)).convert("RGB")
    return pil2tensor(pil)

def get_empty_image(h=1024, w=1024):
    return torch.zeros(1, h, w, 3)


# ===================================================================
#  1. GPT-Image æ–‡ç”Ÿå›¾
# ===================================================================
class GPTImageGenerate:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦GPT-Image æ–‡ç”Ÿå›¾\n"
        "æ”¯æŒ gpt-image-1.5ï¼Œè‡ªåŠ¨åˆ†æ‰¹è·å–å¤šå¼ å›¾"
    )

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_url": ("STRING", {"default": "https://ai.t8star.cn/v1/images/generations"}),
                "api_key": ("STRING", {"default": "", "placeholder": "sk-***"}),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "model": ("STRING", {"default": "gpt-image-1.5", "placeholder": "gpt-image-1.5"}),
                "size": ([
                    "1024x1024 (æ­£æ–¹å½¢)",
                    "1536x1024 (æ¨ªç‰ˆ)", 
                    "1024x1536 (ç«–ç‰ˆ)",
                    "auto (è‡ªåŠ¨)"
                ], {"default": "1024x1024 (æ­£æ–¹å½¢)"}),
                "n": ("INT", {"default": 1, "min": 1, "max": 10}),
                "quality": ([
                    "auto (è‡ªåŠ¨)",
                    "high (é«˜)",
                    "medium (ä¸­)",
                    "low (ä½)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
            },
            "optional": {
                "background": ([
                    "auto (è‡ªåŠ¨)",
                    "transparent (é€æ˜)",
                    "opaque (ä¸é€æ˜)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
                "output_format": (["jpeg", "png", "webp"], {"default": "jpeg"}),
                "output_compression": ("INT", {"default": 90, "min": 0, "max": 100}),
                "moderation": ([
                    "auto (è‡ªåŠ¨)",
                    "low (å®½æ¾)"
                ], {"default": "low (å®½æ¾)"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("image", "info")
    FUNCTION = "generate"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def parse_option(self, option_str: str) -> str:
        """ä»ä¸­æ–‡æ ‡ç­¾æå–å®é™…å€¼"""
        return option_str.split(" ")[0]

    def process_single_image(self, img_data: dict, index: int) -> torch.Tensor:
        """å¤„ç†å•å¼ å›¾ç‰‡æ•°æ®ï¼Œå¤±è´¥è¿”å› None"""
        try:
            if "b64_json" in img_data and img_data["b64_json"]:
                return decode_b64_to_tensor(img_data["b64_json"])
            elif "url" in img_data and img_data["url"]:
                url = img_data["url"]
                print(f"[GPTImage] å›¾{index+1} ä¸‹è½½URL: {url[:40]}...")
                img_resp = requests.get(url, timeout=60)
                img_resp.raise_for_status()
                pil_img = Image.open(io.BytesIO(img_resp.content)).convert("RGB")
                return pil2tensor(pil_img)
            else:
                print(f"[GPTImage] âš ï¸ å›¾{index+1} æ— æœ‰æ•ˆæ•°æ®")
                return None
        except Exception as e:
            print(f"[GPTImage] âš ï¸ å›¾{index+1} å¤„ç†å¤±è´¥: {e}")
            return None

    def request_single_batch(self, api_url: str, headers: dict, payload: dict, 
                            batch_idx: int, total_batches: int) -> list:
        """å‘é€å•æ¬¡è¯·æ±‚ï¼Œè¿”å› tensor åˆ—è¡¨"""
        try:
            print(f"[GPTImage] ç¬¬ {batch_idx}/{total_batches} æ‰¹è¯·æ±‚ (n={payload['n']})...")
            resp = requests.post(api_url, headers=headers, json=payload, timeout=180)
            resp.raise_for_status()
            data = resp.json()

            if "data" not in data or not isinstance(data["data"], list):
                print(f"[GPTImage] âš ï¸ ç¬¬ {batch_idx} æ‰¹è¿”å›å¼‚å¸¸: {list(data.keys())}")
                return []

            batch_tensors = []
            for idx, img_item in enumerate(data["data"]):
                tensor = self.process_single_image(img_item, idx)
                if tensor is not None:
                    if tensor.dim() == 3:
                        tensor = tensor.unsqueeze(0)
                    batch_tensors.append(tensor)
            
            print(f"[GPTImage] ç¬¬ {batch_idx} æ‰¹æˆåŠŸè·å– {len(batch_tensors)} å¼ ")
            return batch_tensors

        except Exception as e:
            print(f"[GPTImage] âš ï¸ ç¬¬ {batch_idx} æ‰¹è¯·æ±‚å¤±è´¥: {e}")
            return []

    def generate(self, api_url, api_key, prompt, model, size, n, quality,
                 background="auto (è‡ªåŠ¨)", output_format="jpeg", 
                 output_compression=90, moderation="auto (è‡ªåŠ¨)"):
        
        if not api_key.strip():
            print("[GPTImage] âŒ API Key ç¼ºå¤±")
            return (get_empty_image(), "Error: API Key ç¼ºå¤±")

        # è§£æå‚æ•°
        size_val = self.parse_option(size)
        quality_val = self.parse_option(quality)
        bg_val = self.parse_option(background)
        mod_val = self.parse_option(moderation)

        # å¹³å°é™åˆ¶æ¯æ‰¹æœ€å¤š 2 å¼ ï¼Œè®¡ç®—åˆ†æ‰¹
        MAX_PER_BATCH = 2
        total_needed = n
        batches = []
        
        remaining = total_needed
        while remaining > 0:
            current_batch = min(remaining, MAX_PER_BATCH)
            batches.append(current_batch)
            remaining -= current_batch

        print(f"[GPTImage] éœ€è¦ {total_needed} å¼ å›¾ï¼Œåˆ† {len(batches)} æ¬¡è¯·æ±‚: {batches}")

        headers = {
            "Authorization": f"{api_key}",
            "Content-Type": "application/json"
        }

        all_tensors = []
        success_count = 0
        
        # å¾ªç¯å‘é€è¯·æ±‚
        for batch_idx, batch_n in enumerate(batches, 1):
            payload = {
                "model": model.strip(),
                "prompt": prompt,
                "n": batch_n,
                "size": size_val,
                "quality": quality_val,
                "background": bg_val,
                "output_format": output_format,
                "output_compression": output_compression,
                "moderation": mod_val,
            }
            
            batch_tensors = self.request_single_batch(
                api_url, headers, payload, batch_idx, len(batches)
            )
            
            all_tensors.extend(batch_tensors)
            success_count += len(batch_tensors)
            
            # ç®€å•é˜²é€Ÿç‡é™åˆ¶ï¼Œæ¯æ‰¹é—´éš” 0.5 ç§’ï¼ˆæœ€åä¸€æ‰¹ä¸ç”¨ç­‰ï¼‰
            if batch_idx < len(batches):
                time.sleep(0.5)

        # å¦‚æœå…¨éƒ¨å¤±è´¥ï¼Œè¿”å›ç©ºå›¾
        if not all_tensors:
            return (get_empty_image(), "Error: æ‰€æœ‰æ‰¹æ¬¡è¯·æ±‚å‡å¤±è´¥")

        # å¦‚æœæˆåŠŸæ•°é‡ä¸è¶³ï¼Œç”¨é»‘å›¾è¡¥é½ï¼ˆä¿æŒç”¨æˆ·è¦æ±‚çš„ n å¼ ï¼‰
        while len(all_tensors) < total_needed:
            all_tensors.append(get_empty_image(1024, 1536 if "1536" in size_val else 1024))
            print(f"[GPTImage] ç”¨ç©ºç™½å›¾è¡¥é½ 1 å¼ ")

        # åˆå¹¶ä¸º batch: [B, H, W, 3]
        batched = torch.cat(all_tensors[:total_needed], dim=0)  # åªå–å‰ n å¼ ï¼Œé˜²æ­¢ API å¤šç»™
        actual_returned = success_count

        print(f"[GPTImage] âœ… æ€»è®¡æˆåŠŸ {actual_returned}/{total_needed} å¼ ï¼Œbatchå½¢çŠ¶: {batched.shape}")

        info = (
            f"ğŸ¨ GPT-Image Generate | {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"æ¨¡å‹: {model} | å°ºå¯¸: {size_val} | è´¨é‡: {quality_val}\n"
            f"èƒŒæ™¯: {bg_val} | æ ¼å¼: {output_format} | å‹ç¼©: {output_compression}\n"
            f"è¯·æ±‚: {total_needed}å¼  | åˆ†æ‰¹: {len(batches)}æ¬¡ | å®é™…è·å–: {actual_returned}å¼ \n"
            f"æç¤ºè¯: {prompt[:50]}{'...' if len(prompt) > 50 else ''}"
        )
        
        return (batched, info)


# ===================================================================
#  2. GPT-Image å›¾åƒç¼–è¾‘
# ===================================================================
class GPTImageEdit:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦GPT-Image å›¾åƒç¼–è¾‘\n"
        "æ”¯æŒæœ€å¤š16å¼ å‚è€ƒå›¾ï¼Œå•å¼ è¾“å‡ºï¼ˆç¼–è¾‘APIä¸æ”¯æŒnå‚æ•°ï¼‰"
    )

    @classmethod
    def INPUT_TYPES(cls):
        # åŠ¨æ€ç”Ÿæˆ16ä¸ªå›¾åƒè¾“å…¥ç«¯å£
        optional_inputs = {
            f"reference_image_{i}": ("IMAGE",) 
            for i in range(1, 17)
        }
        
        return {
            "required": {
                "api_url": ("STRING", {"default": "https://ai.t8star.cn/v1/images/edits"}),
                "api_key": ("STRING", {"default": "", "placeholder": "sk-***"}),
                "prompt": ("STRING", {"multiline": True, "default": "ç»™äººç‰©æ·»åŠ ä¸€å‰¯å¢¨é•œï¼Œä¿æŒé£æ ¼ä¸€è‡´"}),
                "model": ("STRING", {"default": "gpt-image-1.5"}),
                "size": ([
                    "1024x1024 (æ­£æ–¹å½¢)",
                    "1536x1024 (æ¨ªç‰ˆ)",
                    "1024x1536 (ç«–ç‰ˆ)",
                    "auto (è‡ªåŠ¨)"
                ], {"default": "1024x1024 (æ­£æ–¹å½¢)"}),
            },
            "optional": {
                "quality": ([
                    "auto (è‡ªåŠ¨)",
                    "high (é«˜)",
                    "medium (ä¸­)",
                    "low (ä½)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
                "background": ([
                    "auto (è‡ªåŠ¨)",
                    "transparent (é€æ˜)",
                    "opaque (ä¸é€æ˜)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
                "output_format": (["jpeg", "png", "webp"], {"default": "jpeg"}),
                "output_compression": ("INT", {"default": 90, "min": 0, "max": 100}),
                "input_fidelity": (["low (ä½ä¿çœŸ)", "high (é«˜ä¿çœŸ)"], {"default": "low (ä½ä¿çœŸ)"}),
                **optional_inputs
            }
        }

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("image", "info")
    FUNCTION = "edit"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def parse_option(self, option_str: str) -> str:
        return option_str.split(" ")[0]

    def tensor_to_bytes(self, tensor: torch.Tensor, fmt: str = "PNG") -> bytes:
        """å¼ é‡è½¬ä¸ºå­—èŠ‚æµ"""
        pil = tensor2pil_single(tensor)
        buf = io.BytesIO()
        pil.save(buf, format=fmt)
        buf.seek(0)
        return buf.getvalue()

    def edit(self, api_url, api_key, prompt, model, size, 
             quality="auto (è‡ªåŠ¨)", background="auto (è‡ªåŠ¨)", 
             output_format="jpeg", output_compression=90, 
             input_fidelity="low (ä½ä¿çœŸ)", **kwargs):
        
        if not api_key.strip():
            print("[GPTImageEdit] âŒ API Key ç¼ºå¤±")
            return (get_empty_image(), "Error: API Key ç¼ºå¤±")

        # æ”¶é›†æ‰€æœ‰è¾“å…¥çš„å‚è€ƒå›¾ï¼ˆreference_image_1 åˆ° reference_image_16ï¼‰
        images = []
        for i in range(1, 17):
            key = f"reference_image_{i}"
            if key in kwargs and kwargs[key] is not None:
                images.append(kwargs[key])
        
        if not images:
            print("[GPTImageEdit] âŒ è‡³å°‘éœ€è¦æä¾›1å¼ å‚è€ƒå›¾")
            return (get_empty_image(), "Error: è‡³å°‘éœ€è¦1å¼ å‚è€ƒå›¾")

        print(f"[GPTImageEdit] æ”¶åˆ° {len(images)} å¼ å‚è€ƒå›¾ï¼Œå‡†å¤‡ä¸Šä¼ ...")

        size_val = self.parse_option(size)
        quality_val = self.parse_option(quality)
        bg_val = self.parse_option(background)
        fidelity_val = self.parse_option(input_fidelity)

        # æ„å»º multipart/form-dataï¼Œæ”¯æŒå¤šå›¾ä¸Šä¼ 
        files = []
        for idx, img_tensor in enumerate(images):
            img_bytes = self.tensor_to_bytes(img_tensor, "PNG")
            files.append(
                ("image", (f"input_{idx+1}.png", io.BytesIO(img_bytes), "image/png"))
            )

        data = {
            "model": model.strip(),
            "prompt": prompt,
            "size": size_val,
            "quality": quality_val,
            "background": bg_val,
            "output_format": output_format,
            "output_compression": str(output_compression),
        }
        
        # fidelity ä¸æ”¯æŒ 1-mini
        if "1-mini" not in model:
            data["input_fidelity"] = fidelity_val

        headers = {"Authorization": f"{api_key}"}

        try:
            print(f"[GPTImageEdit] å‘é€è¯·æ±‚: {model} | ä¿çœŸ: {fidelity_val} | ä¸Šä¼  {len(images)} å¼ å›¾")
            resp = requests.post(api_url, headers=headers, data=data, files=files, timeout=180)
            resp.raise_for_status()
            result = resp.json()

            # ğŸ” å…³é”®è°ƒè¯•ï¼šæ‰“å°å®Œæ•´å“åº”
            debug_str = json.dumps(result, ensure_ascii=False, indent=2)[:800]
            print(f"[GPTImageEdit] API åŸå§‹å“åº”:\n{debug_str}...")

            # æ£€æŸ¥é”™è¯¯
            if "error" in result:
                err_msg = result["error"].get("message", "æœªçŸ¥é”™è¯¯")
                print(f"[GPTImageEdit] âŒ API è¿”å›é”™è¯¯: {err_msg}")
                return (get_empty_image(), f"API Error: {err_msg}")

            if "data" not in result or not result["data"]:
                print(f"[GPTImageEdit] âš ï¸ å“åº”æ—  data å­—æ®µï¼Œå®é™…å­—æ®µ: {list(result.keys())}")
                return (get_empty_image(), "Error: å“åº”æ— å›¾åƒæ•°æ®")

            # ç¼–è¾‘APIé€šå¸¸åªè¿”å›1å¼ å›¾ï¼Œå–ç¬¬ä¸€å¼ å¤„ç†
            img_data = result["data"][0]
            
            # å°è¯•å¤šç§æ ¼å¼è§£æ
            tensor = None
            if "b64_json" in img_data and img_data["b64_json"]:
                try:
                    tensor = decode_b64_to_tensor(img_data["b64_json"])
                    print(f"[GPTImageEdit] âœ… è§£ç  base64 æˆåŠŸ")
                except Exception as e:
                    print(f"[GPTImageEdit] âš ï¸ base64 è§£ç å¤±è´¥: {e}")
            
            elif "url" in img_data and img_data["url"]:
                try:
                    url = img_data["url"]
                    print(f"[GPTImageEdit] ä¸‹è½½URL: {url[:50]}...")
                    img_resp = requests.get(url, timeout=60)
                    img_resp.raise_for_status()
                    pil_img = Image.open(io.BytesIO(img_resp.content)).convert("RGB")
                    tensor = pil2tensor(pil_img)
                    print(f"[GPTImageEdit] âœ… URL ä¸‹è½½æˆåŠŸ")
                except Exception as e:
                    print(f"[GPTImageEdit] âš ï¸ URL ä¸‹è½½å¤±è´¥: {e}")
            
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å­—æ®µï¼ˆå¦‚ revised_prompt ç­‰å…ƒæ•°æ®ï¼‰
                print(f"[GPTImageEdit] âš ï¸ æ—  b64_json æˆ– urlï¼Œå¯ç”¨å­—æ®µ: {list(img_data.keys())}")
                return (get_empty_image(), f"Error: æ— æ³•è§£æå›¾åƒï¼Œå­—æ®µ: {list(img_data.keys())}")

            if tensor is None:
                return (get_empty_image(), "Error: å›¾åƒè§£ç å¤±è´¥")

            if tensor.dim() == 3:
                tensor = tensor.unsqueeze(0)

            info = (
                f"âœï¸ GPT-Image Edit | {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"æ¨¡å‹: {model} | å°ºå¯¸: {size_val} | ä¿çœŸ: {fidelity_val}\n"
                f"ä¸Šä¼ : {len(images)}å¼ å‚è€ƒå›¾ | è¾“å‡ºæ ¼å¼: {output_format}\n"
                f"æç¤ºè¯: {prompt[:50]}{'...' if len(prompt) > 50 else ''}"
            )
            
            print(f"[GPTImageEdit] âœ… ç¼–è¾‘æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {tensor.shape}")
            return (tensor, info)

        except requests.exceptions.HTTPError as e:
            err_text = e.response.text if e.response else str(e)
            print(f"[GPTImageEdit] âŒ HTTP é”™è¯¯: {err_text[:200]}")
            return (get_empty_image(), f"HTTP Error: {err_text[:200]}")
        except Exception as e:
            err_msg = f"[GPTImageEdit] âŒ è¯·æ±‚å¤±è´¥: {str(e)}"
            print(err_msg)
            return (get_empty_image(), err_msg)


# ===================================================================
#  3. ã€GPT-Image ç¼–è¾‘æäº¤èŠ‚ç‚¹
# ===================================================================
class GPTImageEditSubmit:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦GPT-Image ç¼–è¾‘æäº¤ | çœŸå¹¶å‘\n"
        "å¯åŠ¨åå°çº¿ç¨‹ï¼Œç«‹å³è¿”å› task_idï¼Œä¸é˜»å¡å·¥ä½œæµ"
    )

    @classmethod
    def INPUT_TYPES(cls):
        optional_inputs = {f"reference_image_{i}": ("IMAGE",) for i in range(1, 17)}
        return {
            "required": {
                "api_url": ("STRING", {"default": "https://ai.t8star.cn/v1/images/edits"}),
                "api_key": ("STRING", {"default": "", "placeholder": "sk-***"}),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "model": ("STRING", {"default": "gpt-image-1.5"}),
                "size": (["1024x1024 (æ­£æ–¹å½¢)", "1536x1024 (æ¨ªç‰ˆ)", "1024x1536 (ç«–ç‰ˆ)", "auto (è‡ªåŠ¨)"], {"default": "1024x1024 (æ­£æ–¹å½¢)"}),
            },
            "optional": {
                "quality": (["auto (è‡ªåŠ¨)", "high (é«˜)", "medium (ä¸­)", "low (ä½)"], {"default": "auto (è‡ªåŠ¨)"}),
                "background": (["auto (è‡ªåŠ¨)", "transparent (é€æ˜)", "opaque (ä¸é€æ˜)"], {"default": "auto (è‡ªåŠ¨)"}),
                "output_format": (["jpeg", "png", "webp"], {"default": "jpeg"}),
                "output_compression": ("INT", {"default": 90, "min": 0, "max": 100}),
                "input_fidelity": (["low (ä½ä¿çœŸ)", "high (é«˜ä¿çœŸ)"], {"default": "low (ä½ä¿çœŸ)"}),
                **optional_inputs
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("task_id", "status")
    FUNCTION = "submit"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def parse_option(self, s): 
        return s.split(" ")[0]

    def submit(self, api_url, api_key, prompt, model, size, **kwargs):
        """
        ç”Ÿæˆ task_idï¼Œå¯åŠ¨åå°çº¿ç¨‹ï¼Œç«‹å³è¿”å›
        å®ç°çœŸæ­£çš„å¹¶å‘ï¼šå¤šä¸ª Submit èŠ‚ç‚¹å¯åŒæ—¶å¯åŠ¨
        """
        if not api_key.strip():
            return ("", "Error: API Key missing")

        # æ”¶é›†å›¾ç‰‡
        images = [kwargs.get(f"reference_image_{i}") for i in range(1, 17) 
                  if kwargs.get(f"reference_image_{i}") is not None]
        
        if not images:
            return ("", "Error: No input images")

        # ç”Ÿæˆå”¯ä¸€ ID å’ŒåŒæ­¥äº‹ä»¶
        task_id = str(uuid.uuid4())
        event = threading.Event()
        
        with _cache_lock:
            _processing_events[task_id] = event

        # å‡†å¤‡å‚æ•°
        size_val = self.parse_option(size)
        quality_val = self.parse_option(kwargs.get("quality", "auto (è‡ªåŠ¨)"))
        bg_val = self.parse_option(kwargs.get("background", "auto (è‡ªåŠ¨)"))
        fidelity_val = self.parse_option(kwargs.get("input_fidelity", "low (ä½ä¿çœŸ)"))
        out_format = kwargs.get("output_format", "jpeg")
        out_compress = kwargs.get("output_compression", 90)

        # åå°ä»»åŠ¡å‡½æ•°
        def worker():
            try:
                files = []
                for idx, img in enumerate(images):
                    pil = tensor2pil_single(img)
                    buf = io.BytesIO()
                    pil.save(buf, format="PNG")
                    buf.seek(0)
                    files.append(("image", (f"ref_{idx}.png", buf, "image/png")))

                data = {
                    "model": model.strip(),
                    "prompt": prompt,
                    "size": size_val,
                    "quality": quality_val,
                    "background": bg_val,
                    "output_format": out_format,
                    "output_compression": str(out_compress),
                }
                if "1-mini" not in model:
                    data["input_fidelity"] = fidelity_val

                print(f"[GPTImageEditSubmit] åå°å¼€å§‹ | task: {task_id[:8]} | å›¾ç‰‡: {len(images)}å¼ ")
                
                resp = requests.post(api_url, headers={"Authorization": api_key}, 
                                   data=data, files=files, timeout=180)
                resp.raise_for_status()
                result = resp.json()

                tensor = None
                if "data" in result and result["data"]:
                    img_data = result["data"][0]
                    if "b64_json" in img_data and img_data["b64_json"]:
                        tensor = decode_b64_to_tensor(img_data["b64_json"])
                        if tensor.dim() == 3:
                            tensor = tensor.unsqueeze(0)
                    elif "url" in img_data and img_data["url"]:
                        r = requests.get(img_data["url"], timeout=60)
                        pil = Image.open(io.BytesIO(r.content)).convert("RGB")
                        tensor = pil2tensor(pil)
                
                if tensor is not None:
                    cache_result(task_id, tensor)
                    print(f"[GPTImageEditSubmit] åå°å®Œæˆ | task: {task_id[:8]} | æˆåŠŸ")
                else:
                    cache_result(task_id, None)
                    print(f"[GPTImageEditSubmit] åå°å®Œæˆ | task: {task_id[:8]} | æ— å›¾åƒ")
                    
            except Exception as e:
                print(f"[GPTImageEditSubmit] åå°å¼‚å¸¸ | task: {task_id[:8]} | {e}")
                cache_result(task_id, None)

        # å¯åŠ¨åå°çº¿ç¨‹ï¼Œç«‹å³è¿”å›
        thread = threading.Thread(target=worker, daemon=True)
        thread.start()
        
        print(f"[GPTImageEditSubmit] å·²æäº¤ | task_id: {task_id[:8]}...")
        return (task_id, "Submitted")


# ===================================================================
#  4. æ”¶é›†èŠ‚ç‚¹
# ===================================================================
class GPTImageEditCollect:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦ä»»åŠ¡æ”¶é›†å™¨ | ç»Ÿä¸€ç­‰å¾…\n"
        "é˜»å¡ç­‰å¾…9ä¸ªä»»åŠ¡å…¨éƒ¨å®Œæˆï¼Œå¤±è´¥å¡«ç©ºç™½å›¾"
    )

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "optional": {
                "task_id_1": ("STRING", {"forceInput": True}),
                "task_id_2": ("STRING", {"forceInput": True}),
                "task_id_3": ("STRING", {"forceInput": True}),
                "task_id_4": ("STRING", {"forceInput": True}),
                "task_id_5": ("STRING", {"forceInput": True}),
                "task_id_6": ("STRING", {"forceInput": True}),
                "task_id_7": ("STRING", {"forceInput": True}),
                "task_id_8": ("STRING", {"forceInput": True}),
                "task_id_9": ("STRING", {"forceInput": True}),
            }
        }

    RETURN_TYPES = ("IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE")
    RETURN_NAMES = ("image_1", "image_2", "image_3", "image_4", "image_5", "image_6", "image_7", "image_8", "image_9")
    FUNCTION = "collect"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def collect(self, task_id_1=None, task_id_2=None, task_id_3=None, 
                task_id_4=None, task_id_5=None, task_id_6=None,
                task_id_7=None, task_id_8=None, task_id_9=None):
        """
        ç»Ÿä¸€ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆæœ€å¤šç­‰300ç§’ï¼‰
        å¦‚æœæŸé€šé“æœªè¿æ¥ï¼Œä¹Ÿè¿”å›ç©ºå›¾å ä½
        """
        task_ids = [task_id_1, task_id_2, task_id_3, task_id_4, task_id_5, 
                   task_id_6, task_id_7, task_id_8, task_id_9]
        
        print(f"[Collect] å¼€å§‹æ”¶é›†ï¼Œæ£€æŸ¥9ä¸ªé€šé“...")
        
        # å…ˆç»Ÿè®¡æœ‰æ•ˆçš„ä»»åŠ¡
        valid_tasks = [(i, tid) for i, tid in enumerate(task_ids, 1) if tid]
        if not valid_tasks:
            print("[Collect] æ— æœ‰æ•ˆä»»åŠ¡ï¼Œå…¨éƒ¨è¿”å›ç©ºå›¾")
            return tuple([get_empty_image() for _ in range(9)])
        
        print(f"[Collect] æœ‰æ•ˆä»»åŠ¡: {len(valid_tasks)}ä¸ªï¼Œå¼€å§‹ç­‰å¾…...")
        
        # ç»Ÿä¸€ç­‰å¾…æ‰€æœ‰æœ‰æ•ˆä»»åŠ¡ï¼ˆæœ€å¤š300ç§’ï¼‰
        max_wait = 300  # 5åˆ†é’Ÿè¶…æ—¶
        start_time = time.time()
        all_done = False
        
        while not all_done and (time.time() - start_time) < max_wait:
            all_done = True
            for idx, tid in valid_tasks:
                if get_result(tid) is None:
                    # è¿˜åœ¨å¤„ç†ä¸­
                    all_done = False
                    break
            
            if not all_done:
                time.sleep(0.5)  # è½®è¯¢é—´éš”
        
        # æ”¶é›†ç»“æœ
        results = []
        for i, tid in enumerate(task_ids, 1):
            if not tid:
                results.append(get_empty_image())
                print(f"[Collect] é€šé“{i}: æœªè¿æ¥")
            else:
                tensor = get_result(tid)
                if tensor is not None:
                    results.append(tensor)
                    print(f"[Collect] é€šé“{i}: æˆåŠŸ ({tid[:8]})")
                else:
                    # å¤±è´¥æˆ–è¶…æ—¶ï¼Œæ ¹æ®è¾“å…¥å›¾æ¨æ–­å°ºå¯¸ï¼Ÿè¿™é‡Œç»Ÿä¸€ç”¨1024x1024
                    results.append(get_empty_image())
                    print(f"[Collect] é€šé“{i}: å¤±è´¥/è¶…æ—¶ ({tid[:8]})")
        
        print(f"[Collect] æ”¶é›†å®Œæˆï¼Œè¾“å‡º9å¼ å›¾")
        return tuple(results)


# ===================================================================
#  ç»Ÿä¸€æ³¨å†Œ
# ===================================================================
register_node(GPTImageGenerate, "GPTImage_Generate_mmx")
register_node(GPTImageEdit, "GPTImage_Edit_mmx")
register_node(GPTImageEditSubmit, "GPTImage_Edit_Submit_mmx")
register_node(GPTImageEditCollect, "GPTImage_Edit_Collect_mmx")
