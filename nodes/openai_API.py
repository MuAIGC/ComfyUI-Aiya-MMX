# ~/ComfyUI/custom_nodes/ComfyUI-Aiya-MMX/nodes/openai_API.py
from __future__ import annotations
import io
import json
import base64
import time
import requests
import torch
from pathlib import Path
from PIL import Image
from ..register import register_node
from ..mmx_utils import pil2tensor, tensor2pil

# ---------- é€šç”¨å·¥å…· ----------
def tensor2pil_single(t: torch.Tensor) -> Image.Image:
    """ä¸¥æ ¼å•å¼ è½¬æ¢"""
    if t.dim() == 4:
        t = t.squeeze(0)
    t = (t.clamp(0, 1) * 255).byte().cpu()
    return Image.fromarray(t.numpy())

def decode_b64_to_tensor(b64_str: str):
    """base64 è½¬å¼ é‡"""
    img_bytes = base64.b64decode(b64_str)
    pil = Image.open(io.BytesIO(img_bytes)).convert("RGB")
    return pil2tensor(pil)

def get_empty_image(h=1024, w=1024):
    """è¿”å›æŒ‡å®šå°ºå¯¸é»‘å›¾"""
    return torch.zeros(1, h, w, 3)


# ===================================================================
#  1. GPT-Image æ–‡ç”Ÿå›¾ï¼ˆæ”¯æŒè‡ªåŠ¨åˆ†æ‰¹è¯·æ±‚çªç ´å¹³å°é™åˆ¶ï¼‰
# ===================================================================
class GPTImageGenerate:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦GPT-Image æ–‡ç”Ÿå›¾\n"
        "æ”¯æŒ gpt-image-1.5ï¼Œè‡ªåŠ¨åˆ†æ‰¹è·å–å¤šå¼ å›¾"
    )

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_url": ("STRING", {"default": "https://ai.t8star.cn/v1/images/generations"}),
                "api_key": ("STRING", {"default": "", "placeholder": "sk-***"}),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "model": ("STRING", {"default": "gpt-image-1.5", "placeholder": "gpt-image-1.5"}),
                "size": ([
                    "1024x1024 (æ­£æ–¹å½¢)",
                    "1536x1024 (æ¨ªç‰ˆ)", 
                    "1024x1536 (ç«–ç‰ˆ)",
                    "auto (è‡ªåŠ¨)"
                ], {"default": "1024x1024 (æ­£æ–¹å½¢)"}),
                "n": ("INT", {"default": 1, "min": 1, "max": 10}),
                "quality": ([
                    "auto (è‡ªåŠ¨)",
                    "high (é«˜)",
                    "medium (ä¸­)",
                    "low (ä½)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
            },
            "optional": {
                "background": ([
                    "auto (è‡ªåŠ¨)",
                    "transparent (é€æ˜)",
                    "opaque (ä¸é€æ˜)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
                "output_format": (["jpeg", "png", "webp"], {"default": "jpeg"}),
                "output_compression": ("INT", {"default": 90, "min": 0, "max": 100}),
                "moderation": ([
                    "auto (è‡ªåŠ¨)",
                    "low (å®½æ¾)"
                ], {"default": "low (å®½æ¾)"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("image", "info")
    FUNCTION = "generate"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def parse_option(self, option_str: str) -> str:
        """ä»ä¸­æ–‡æ ‡ç­¾æå–å®é™…å€¼"""
        return option_str.split(" ")[0]

    def process_single_image(self, img_data: dict, index: int) -> torch.Tensor:
        """å¤„ç†å•å¼ å›¾ç‰‡æ•°æ®ï¼Œå¤±è´¥è¿”å› None"""
        try:
            if "b64_json" in img_data and img_data["b64_json"]:
                return decode_b64_to_tensor(img_data["b64_json"])
            elif "url" in img_data and img_data["url"]:
                url = img_data["url"]
                print(f"[GPTImage] å›¾{index+1} ä¸‹è½½URL: {url[:40]}...")
                img_resp = requests.get(url, timeout=60)
                img_resp.raise_for_status()
                pil_img = Image.open(io.BytesIO(img_resp.content)).convert("RGB")
                return pil2tensor(pil_img)
            else:
                print(f"[GPTImage] âš ï¸ å›¾{index+1} æ— æœ‰æ•ˆæ•°æ®")
                return None
        except Exception as e:
            print(f"[GPTImage] âš ï¸ å›¾{index+1} å¤„ç†å¤±è´¥: {e}")
            return None

    def request_single_batch(self, api_url: str, headers: dict, payload: dict, 
                            batch_idx: int, total_batches: int) -> list:
        """å‘é€å•æ¬¡è¯·æ±‚ï¼Œè¿”å› tensor åˆ—è¡¨"""
        try:
            print(f"[GPTImage] ç¬¬ {batch_idx}/{total_batches} æ‰¹è¯·æ±‚ (n={payload['n']})...")
            resp = requests.post(api_url, headers=headers, json=payload, timeout=180)
            resp.raise_for_status()
            data = resp.json()

            if "data" not in data or not isinstance(data["data"], list):
                print(f"[GPTImage] âš ï¸ ç¬¬ {batch_idx} æ‰¹è¿”å›å¼‚å¸¸: {list(data.keys())}")
                return []

            batch_tensors = []
            for idx, img_item in enumerate(data["data"]):
                tensor = self.process_single_image(img_item, idx)
                if tensor is not None:
                    if tensor.dim() == 3:
                        tensor = tensor.unsqueeze(0)
                    batch_tensors.append(tensor)
            
            print(f"[GPTImage] ç¬¬ {batch_idx} æ‰¹æˆåŠŸè·å– {len(batch_tensors)} å¼ ")
            return batch_tensors

        except Exception as e:
            print(f"[GPTImage] âš ï¸ ç¬¬ {batch_idx} æ‰¹è¯·æ±‚å¤±è´¥: {e}")
            return []

    def generate(self, api_url, api_key, prompt, model, size, n, quality,
                 background="auto (è‡ªåŠ¨)", output_format="jpeg", 
                 output_compression=90, moderation="auto (è‡ªåŠ¨)"):
        
        if not api_key.strip():
            print("[GPTImage] âŒ API Key ç¼ºå¤±")
            return (get_empty_image(), "Error: API Key ç¼ºå¤±")

        # è§£æå‚æ•°
        size_val = self.parse_option(size)
        quality_val = self.parse_option(quality)
        bg_val = self.parse_option(background)
        mod_val = self.parse_option(moderation)

        # å¹³å°é™åˆ¶æ¯æ‰¹æœ€å¤š 2 å¼ ï¼Œè®¡ç®—åˆ†æ‰¹
        MAX_PER_BATCH = 2
        total_needed = n
        batches = []
        
        remaining = total_needed
        while remaining > 0:
            current_batch = min(remaining, MAX_PER_BATCH)
            batches.append(current_batch)
            remaining -= current_batch

        print(f"[GPTImage] éœ€è¦ {total_needed} å¼ å›¾ï¼Œåˆ† {len(batches)} æ¬¡è¯·æ±‚: {batches}")

        headers = {
            "Authorization": f"{api_key}",
            "Content-Type": "application/json"
        }

        all_tensors = []
        success_count = 0
        
        # å¾ªç¯å‘é€è¯·æ±‚
        for batch_idx, batch_n in enumerate(batches, 1):
            payload = {
                "model": model.strip(),
                "prompt": prompt,
                "n": batch_n,
                "size": size_val,
                "quality": quality_val,
                "background": bg_val,
                "output_format": output_format,
                "output_compression": output_compression,
                "moderation": mod_val,
            }
            
            batch_tensors = self.request_single_batch(
                api_url, headers, payload, batch_idx, len(batches)
            )
            
            all_tensors.extend(batch_tensors)
            success_count += len(batch_tensors)
            
            # ç®€å•é˜²é€Ÿç‡é™åˆ¶ï¼Œæ¯æ‰¹é—´éš” 0.5 ç§’ï¼ˆæœ€åä¸€æ‰¹ä¸ç”¨ç­‰ï¼‰
            if batch_idx < len(batches):
                time.sleep(0.5)

        # å¦‚æœå…¨éƒ¨å¤±è´¥ï¼Œè¿”å›ç©ºå›¾
        if not all_tensors:
            return (get_empty_image(), "Error: æ‰€æœ‰æ‰¹æ¬¡è¯·æ±‚å‡å¤±è´¥")

        # å¦‚æœæˆåŠŸæ•°é‡ä¸è¶³ï¼Œç”¨é»‘å›¾è¡¥é½ï¼ˆä¿æŒç”¨æˆ·è¦æ±‚çš„ n å¼ ï¼‰
        while len(all_tensors) < total_needed:
            all_tensors.append(get_empty_image(1024, 1536 if "1536" in size_val else 1024))
            print(f"[GPTImage] ç”¨ç©ºç™½å›¾è¡¥é½ 1 å¼ ")

        # åˆå¹¶ä¸º batch: [B, H, W, 3]
        batched = torch.cat(all_tensors[:total_needed], dim=0)  # åªå–å‰ n å¼ ï¼Œé˜²æ­¢ API å¤šç»™
        actual_returned = success_count

        print(f"[GPTImage] âœ… æ€»è®¡æˆåŠŸ {actual_returned}/{total_needed} å¼ ï¼Œbatchå½¢çŠ¶: {batched.shape}")

        info = (
            f"ğŸ¨ GPT-Image Generate | {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"æ¨¡å‹: {model} | å°ºå¯¸: {size_val} | è´¨é‡: {quality_val}\n"
            f"èƒŒæ™¯: {bg_val} | æ ¼å¼: {output_format} | å‹ç¼©: {output_compression}\n"
            f"è¯·æ±‚: {total_needed}å¼  | åˆ†æ‰¹: {len(batches)}æ¬¡ | å®é™…è·å–: {actual_returned}å¼ \n"
            f"æç¤ºè¯: {prompt[:50]}{'...' if len(prompt) > 50 else ''}"
        )
        
        return (batched, info)


# ===================================================================
#  2. GPT-Image å›¾åƒç¼–è¾‘ï¼ˆä¿®å¤ç‰ˆï¼šæ”¯æŒ16å¼ å›¾è¾“å…¥ï¼Œä¿®å¤å“åº”è§£æï¼‰
# ===================================================================
class GPTImageEdit:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦GPT-Image å›¾åƒç¼–è¾‘\n"
        "æ”¯æŒæœ€å¤š16å¼ å‚è€ƒå›¾ï¼Œå•å¼ è¾“å‡ºï¼ˆç¼–è¾‘APIä¸æ”¯æŒnå‚æ•°ï¼‰"
    )

    @classmethod
    def INPUT_TYPES(cls):
        # åŠ¨æ€ç”Ÿæˆ16ä¸ªå›¾åƒè¾“å…¥ç«¯å£
        optional_inputs = {
            f"reference_image_{i}": ("IMAGE",) 
            for i in range(1, 17)
        }
        
        return {
            "required": {
                "api_url": ("STRING", {"default": "https://ai.t8star.cn/v1/images/edits"}),
                "api_key": ("STRING", {"default": "", "placeholder": "sk-***"}),
                "prompt": ("STRING", {"multiline": True, "default": "ç»™äººç‰©æ·»åŠ ä¸€å‰¯å¢¨é•œï¼Œä¿æŒé£æ ¼ä¸€è‡´"}),
                "model": ("STRING", {"default": "gpt-image-1.5"}),
                "size": ([
                    "1024x1024 (æ­£æ–¹å½¢)",
                    "1536x1024 (æ¨ªç‰ˆ)",
                    "1024x1536 (ç«–ç‰ˆ)",
                    "auto (è‡ªåŠ¨)"
                ], {"default": "1024x1024 (æ­£æ–¹å½¢)"}),
            },
            "optional": {
                "quality": ([
                    "auto (è‡ªåŠ¨)",
                    "high (é«˜)",
                    "medium (ä¸­)",
                    "low (ä½)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
                "background": ([
                    "auto (è‡ªåŠ¨)",
                    "transparent (é€æ˜)",
                    "opaque (ä¸é€æ˜)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
                "output_format": (["jpeg", "png", "webp"], {"default": "jpeg"}),
                "output_compression": ("INT", {"default": 90, "min": 0, "max": 100}),
                "input_fidelity": (["low (ä½ä¿çœŸ)", "high (é«˜ä¿çœŸ)"], {"default": "low (ä½ä¿çœŸ)"}),
                **optional_inputs
            }
        }

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("image", "info")
    FUNCTION = "edit"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def parse_option(self, option_str: str) -> str:
        return option_str.split(" ")[0]

    def tensor_to_bytes(self, tensor: torch.Tensor, fmt: str = "PNG") -> bytes:
        """å¼ é‡è½¬ä¸ºå­—èŠ‚æµ"""
        pil = tensor2pil_single(tensor)
        buf = io.BytesIO()
        pil.save(buf, format=fmt)
        buf.seek(0)
        return buf.getvalue()

    def edit(self, api_url, api_key, prompt, model, size, 
             quality="auto (è‡ªåŠ¨)", background="auto (è‡ªåŠ¨)", 
             output_format="jpeg", output_compression=90, 
             input_fidelity="low (ä½ä¿çœŸ)", **kwargs):
        
        if not api_key.strip():
            print("[GPTImageEdit] âŒ API Key ç¼ºå¤±")
            return (get_empty_image(), "Error: API Key ç¼ºå¤±")

        # æ”¶é›†æ‰€æœ‰è¾“å…¥çš„å‚è€ƒå›¾ï¼ˆreference_image_1 åˆ° reference_image_16ï¼‰
        images = []
        for i in range(1, 17):
            key = f"reference_image_{i}"
            if key in kwargs and kwargs[key] is not None:
                images.append(kwargs[key])
        
        if not images:
            print("[GPTImageEdit] âŒ è‡³å°‘éœ€è¦æä¾›1å¼ å‚è€ƒå›¾")
            return (get_empty_image(), "Error: è‡³å°‘éœ€è¦1å¼ å‚è€ƒå›¾")

        print(f"[GPTImageEdit] æ”¶åˆ° {len(images)} å¼ å‚è€ƒå›¾ï¼Œå‡†å¤‡ä¸Šä¼ ...")

        size_val = self.parse_option(size)
        quality_val = self.parse_option(quality)
        bg_val = self.parse_option(background)
        fidelity_val = self.parse_option(input_fidelity)

        # æ„å»º multipart/form-dataï¼Œæ”¯æŒå¤šå›¾ä¸Šä¼ 
        files = []
        for idx, img_tensor in enumerate(images):
            img_bytes = self.tensor_to_bytes(img_tensor, "PNG")
            files.append(
                ("image", (f"input_{idx+1}.png", io.BytesIO(img_bytes), "image/png"))
            )

        data = {
            "model": model.strip(),
            "prompt": prompt,
            "size": size_val,
            "quality": quality_val,
            "background": bg_val,
            "output_format": output_format,
            "output_compression": str(output_compression),
        }
        
        # fidelity ä¸æ”¯æŒ 1-mini
        if "1-mini" not in model:
            data["input_fidelity"] = fidelity_val

        headers = {"Authorization": f"{api_key}"}

        try:
            print(f"[GPTImageEdit] å‘é€è¯·æ±‚: {model} | ä¿çœŸ: {fidelity_val} | ä¸Šä¼  {len(images)} å¼ å›¾")
            resp = requests.post(api_url, headers=headers, data=data, files=files, timeout=180)
            resp.raise_for_status()
            result = resp.json()

            # ğŸ” å…³é”®è°ƒè¯•ï¼šæ‰“å°å®Œæ•´å“åº”
            debug_str = json.dumps(result, ensure_ascii=False, indent=2)[:800]
            print(f"[GPTImageEdit] API åŸå§‹å“åº”:\n{debug_str}...")

            # æ£€æŸ¥é”™è¯¯
            if "error" in result:
                err_msg = result["error"].get("message", "æœªçŸ¥é”™è¯¯")
                print(f"[GPTImageEdit] âŒ API è¿”å›é”™è¯¯: {err_msg}")
                return (get_empty_image(), f"API Error: {err_msg}")

            if "data" not in result or not result["data"]:
                print(f"[GPTImageEdit] âš ï¸ å“åº”æ—  data å­—æ®µï¼Œå®é™…å­—æ®µ: {list(result.keys())}")
                return (get_empty_image(), "Error: å“åº”æ— å›¾åƒæ•°æ®")

            # ç¼–è¾‘APIé€šå¸¸åªè¿”å›1å¼ å›¾ï¼Œå–ç¬¬ä¸€å¼ å¤„ç†
            img_data = result["data"][0]
            
            # å°è¯•å¤šç§æ ¼å¼è§£æ
            tensor = None
            if "b64_json" in img_data and img_data["b64_json"]:
                try:
                    tensor = decode_b64_to_tensor(img_data["b64_json"])
                    print(f"[GPTImageEdit] âœ… è§£ç  base64 æˆåŠŸ")
                except Exception as e:
                    print(f"[GPTImageEdit] âš ï¸ base64 è§£ç å¤±è´¥: {e}")
            
            elif "url" in img_data and img_data["url"]:
                try:
                    url = img_data["url"]
                    print(f"[GPTImageEdit] ä¸‹è½½URL: {url[:50]}...")
                    img_resp = requests.get(url, timeout=60)
                    img_resp.raise_for_status()
                    pil_img = Image.open(io.BytesIO(img_resp.content)).convert("RGB")
                    tensor = pil2tensor(pil_img)
                    print(f"[GPTImageEdit] âœ… URL ä¸‹è½½æˆåŠŸ")
                except Exception as e:
                    print(f"[GPTImageEdit] âš ï¸ URL ä¸‹è½½å¤±è´¥: {e}")
            
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å­—æ®µï¼ˆå¦‚ revised_prompt ç­‰å…ƒæ•°æ®ï¼‰
                print(f"[GPTImageEdit] âš ï¸ æ—  b64_json æˆ– urlï¼Œå¯ç”¨å­—æ®µ: {list(img_data.keys())}")
                return (get_empty_image(), f"Error: æ— æ³•è§£æå›¾åƒï¼Œå­—æ®µ: {list(img_data.keys())}")

            if tensor is None:
                return (get_empty_image(), "Error: å›¾åƒè§£ç å¤±è´¥")

            if tensor.dim() == 3:
                tensor = tensor.unsqueeze(0)

            info = (
                f"âœï¸ GPT-Image Edit | {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"æ¨¡å‹: {model} | å°ºå¯¸: {size_val} | ä¿çœŸ: {fidelity_val}\n"
                f"ä¸Šä¼ : {len(images)}å¼ å‚è€ƒå›¾ | è¾“å‡ºæ ¼å¼: {output_format}\n"
                f"æç¤ºè¯: {prompt[:50]}{'...' if len(prompt) > 50 else ''}"
            )
            
            print(f"[GPTImageEdit] âœ… ç¼–è¾‘æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {tensor.shape}")
            return (tensor, info)

        except requests.exceptions.HTTPError as e:
            err_text = e.response.text if e.response else str(e)
            print(f"[GPTImageEdit] âŒ HTTP é”™è¯¯: {err_text[:200]}")
            return (get_empty_image(), f"HTTP Error: {err_text[:200]}")
        except Exception as e:
            err_msg = f"[GPTImageEdit] âŒ è¯·æ±‚å¤±è´¥: {str(e)}"
            print(err_msg)
            return (get_empty_image(), err_msg)


# ===================================================================
#  ç»Ÿä¸€æ³¨å†Œ
# ===================================================================
register_node(GPTImageGenerate, "GPTImage_Generate_mmx")
register_node(GPTImageEdit, "GPTImage_Edit_mmx")