# ~/ComfyUI/custom_nodes/ComfyUI-Aiya-MMX/nodes/openai_API.py
from __future__ import annotations
import io
import json
import base64
import time
import uuid
import threading
import requests
import torch
from PIL import Image
from ..register import register_node
from ..mmx_utils import pil2tensor, tensor2pil

# ---------- å…¨å±€å¹¶å‘ç¼“å­˜ ----------
_result_cache = {}
_processing_events = {}  # task_id -> threading.Event()
_cache_lock = threading.Lock()
_CACHE_TTL = 600  # 10åˆ†é’Ÿè¿‡æœŸ

def _cleanup_cache():
    now = time.time()
    expired = [k for k, (ts, _) in _result_cache.items() if now - ts > _CACHE_TTL]
    for k in expired:
        del _result_cache[k]

def cache_result(task_id: str, tensor: torch.Tensor | None):
    """å­˜å…¥ç»“æœå¹¶é€šçŸ¥ç­‰å¾…è€…"""
    with _cache_lock:
        _cleanup_cache()
        _result_cache[task_id] = (time.time(), tensor)
        if task_id in _processing_events:
            _processing_events[task_id].set()

def get_result(task_id: str) -> torch.Tensor | None:
    """è·å–ç»“æœï¼ˆéé˜»å¡ï¼‰"""
    if not task_id:
        return None
    with _cache_lock:
        if task_id in _result_cache:
            ts, tensor = _result_cache[task_id]
            if time.time() - ts < _CACHE_TTL:
                return tensor
            else:
                del _result_cache[task_id]
        return None

def wait_for_result(task_id: str, timeout: float = 300) -> torch.Tensor | None:
    """é˜»å¡ç­‰å¾…ç»“æœ"""
    if not task_id:
        return None
    
    # å…ˆæ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
    result = get_result(task_id)
    if result is not None:
        return result
    
    # ç­‰å¾… Event
    event = None
    with _cache_lock:
        if task_id in _processing_events:
            event = _processing_events[task_id]
    
    if event:
        event.wait(timeout)
        return get_result(task_id)
    return None


# ---------- é€šç”¨å·¥å…· ----------
def tensor2pil_single(t: torch.Tensor) -> Image.Image:
    if t.dim() == 4:
        t = t.squeeze(0)
    t = (t.clamp(0, 1) * 255).byte().cpu()
    return Image.fromarray(t.numpy())

def decode_b64_to_tensor(b64_str: str):
    img_bytes = base64.b64decode(b64_str)
    pil = Image.open(io.BytesIO(img_bytes)).convert("RGB")
    return pil2tensor(pil)

def get_empty_image(h=1024, w=1024):
    return torch.zeros(1, h, w, 3)


# ===================================================================
#  1. GPT-Image æ–‡ç”Ÿå›¾
# ===================================================================
class GPTImageGenerate:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦GPT-Image æ–‡ç”Ÿå›¾\n"
        "æ”¯æŒ gpt-image-1.5ï¼Œè‡ªåŠ¨åˆ†æ‰¹è·å–å¤šå¼ å›¾"
    )

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_url": ("STRING", {"default": "https://ai.t8star.cn/v1/images/generations "}),
                "api_key": ("STRING", {"default": "", "placeholder": "sk-***"}),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "model": ("STRING", {"default": "gpt-image-1.5", "placeholder": "gpt-image-1.5"}),
                "size": ([
                    "1024x1024 (æ­£æ–¹å½¢)",
                    "1536x1024 (æ¨ªç‰ˆ)", 
                    "1024x1536 (ç«–ç‰ˆ)",
                    "auto (è‡ªåŠ¨)"
                ], {"default": "1024x1024 (æ­£æ–¹å½¢)"}),
                "n": ("INT", {"default": 1, "min": 1, "max": 10}),
                "quality": ([
                    "auto (è‡ªåŠ¨)",
                    "high (é«˜)",
                    "medium (ä¸­)",
                    "low (ä½)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
            },
            "optional": {
                "background": ([
                    "auto (è‡ªåŠ¨)",
                    "transparent (é€æ˜)",
                    "opaque (ä¸é€æ˜)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
                "output_format": (["jpeg", "png", "webp"], {"default": "jpeg"}),
                "output_compression": ("INT", {"default": 90, "min": 0, "max": 100}),
                "moderation": ([
                    "auto (è‡ªåŠ¨)",
                    "low (å®½æ¾)"
                ], {"default": "low (å®½æ¾)"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("image", "info")
    FUNCTION = "generate"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def parse_option(self, option_str: str) -> str:
        """ä»ä¸­æ–‡æ ‡ç­¾æå–å®é™…å€¼"""
        return option_str.split(" ")[0]

    def process_single_image(self, img_data: dict, index: int) -> torch.Tensor:
        """å¤„ç†å•å¼ å›¾ç‰‡æ•°æ®ï¼Œå¤±è´¥è¿”å› None"""
        try:
            if "b64_json" in img_data and img_data["b64_json"]:
                return decode_b64_to_tensor(img_data["b64_json"])
            elif "url" in img_data and img_data["url"]:
                url = img_data["url"]
                print(f"[GPTImage] å›¾{index+1} ä¸‹è½½URL: {url[:40]}...")
                img_resp = requests.get(url, timeout=60)
                img_resp.raise_for_status()
                pil_img = Image.open(io.BytesIO(img_resp.content)).convert("RGB")
                return pil2tensor(pil_img)
            else:
                print(f"[GPTImage] âš ï¸ å›¾{index+1} æ— æœ‰æ•ˆæ•°æ®")
                return None
        except Exception as e:
            print(f"[GPTImage] âš ï¸ å›¾{index+1} å¤„ç†å¤±è´¥: {e}")
            return None

    def request_single_batch(self, api_url: str, headers: dict, payload: dict, 
                            batch_idx: int, total_batches: int) -> list:
        """å‘é€å•æ¬¡è¯·æ±‚ï¼Œè¿”å› tensor åˆ—è¡¨"""
        try:
            print(f"[GPTImage] ç¬¬ {batch_idx}/{total_batches} æ‰¹è¯·æ±‚ (n={payload['n']})...")
            resp = requests.post(api_url, headers=headers, json=payload, timeout=180)
            resp.raise_for_status()
            data = resp.json()

            if "data" not in data or not isinstance(data["data"], list):
                print(f"[GPTImage] âš ï¸ ç¬¬ {batch_idx} æ‰¹è¿”å›å¼‚å¸¸: {list(data.keys())}")
                return []

            batch_tensors = []
            for idx, img_item in enumerate(data["data"]):
                tensor = self.process_single_image(img_item, idx)
                if tensor is not None:
                    if tensor.dim() == 3:
                        tensor = tensor.unsqueeze(0)
                    batch_tensors.append(tensor)
            
            print(f"[GPTImage] ç¬¬ {batch_idx} æ‰¹æˆåŠŸè·å– {len(batch_tensors)} å¼ ")
            return batch_tensors

        except Exception as e:
            print(f"[GPTImage] âš ï¸ ç¬¬ {batch_idx} æ‰¹è¯·æ±‚å¤±è´¥: {e}")
            return []

    def generate(self, api_url, api_key, prompt, model, size, n, quality,
                 background="auto (è‡ªåŠ¨)", output_format="jpeg", 
                 output_compression=90, moderation="auto (è‡ªåŠ¨)"):
        
        if not api_key.strip():
            print("[GPTImage] âŒ API Key ç¼ºå¤±")
            return (get_empty_image(), "Error: API Key ç¼ºå¤±")

        # è§£æå‚æ•°
        size_val = self.parse_option(size)
        quality_val = self.parse_option(quality)
        bg_val = self.parse_option(background)
        mod_val = self.parse_option(moderation)

        # å¹³å°é™åˆ¶æ¯æ‰¹æœ€å¤š 2 å¼ ï¼Œè®¡ç®—åˆ†æ‰¹
        MAX_PER_BATCH = 2
        total_needed = n
        batches = []
        
        remaining = total_needed
        while remaining > 0:
            current_batch = min(remaining, MAX_PER_BATCH)
            batches.append(current_batch)
            remaining -= current_batch

        print(f"[GPTImage] éœ€è¦ {total_needed} å¼ å›¾ï¼Œåˆ† {len(batches)} æ¬¡è¯·æ±‚: {batches}")

        headers = {
            "Authorization": f"{api_key}",
            "Content-Type": "application/json"
        }

        all_tensors = []
        success_count = 0
        
        # å¾ªç¯å‘é€è¯·æ±‚
        for batch_idx, batch_n in enumerate(batches, 1):
            payload = {
                "model": model.strip(),
                "prompt": prompt,
                "n": batch_n,
                "size": size_val,
                "quality": quality_val,
                "background": bg_val,
                "output_format": output_format,
                "output_compression": output_compression,
                "moderation": mod_val,
            }
            
            batch_tensors = self.request_single_batch(
                api_url, headers, payload, batch_idx, len(batches)
            )
            
            all_tensors.extend(batch_tensors)
            success_count += len(batch_tensors)
            
            # ç®€å•é˜²é€Ÿç‡é™åˆ¶ï¼Œæ¯æ‰¹é—´éš” 0.5 ç§’ï¼ˆæœ€åä¸€æ‰¹ä¸ç”¨ç­‰ï¼‰
            if batch_idx < len(batches):
                time.sleep(0.5)

        # å¦‚æœå…¨éƒ¨å¤±è´¥ï¼Œè¿”å›ç©ºå›¾
        if not all_tensors:
            return (get_empty_image(), "Error: æ‰€æœ‰æ‰¹æ¬¡è¯·æ±‚å‡å¤±è´¥")

        # å¦‚æœæˆåŠŸæ•°é‡ä¸è¶³ï¼Œç”¨é»‘å›¾è¡¥é½ï¼ˆä¿æŒç”¨æˆ·è¦æ±‚çš„ n å¼ ï¼‰
        while len(all_tensors) < total_needed:
            all_tensors.append(get_empty_image(1024, 1536 if "1536" in size_val else 1024))
            print(f"[GPTImage] ç”¨ç©ºç™½å›¾è¡¥é½ 1 å¼ ")

        # åˆå¹¶ä¸º batch: [B, H, W, 3]
        batched = torch.cat(all_tensors[:total_needed], dim=0)  # åªå–å‰ n å¼ ï¼Œé˜²æ­¢ API å¤šç»™
        actual_returned = success_count

        print(f"[GPTImage] âœ… æ€»è®¡æˆåŠŸ {actual_returned}/{total_needed} å¼ ï¼Œbatchå½¢çŠ¶: {batched.shape}")

        info = (
            f"ğŸ¨ GPT-Image Generate | {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"æ¨¡å‹: {model} | å°ºå¯¸: {size_val} | è´¨é‡: {quality_val}\n"
            f"èƒŒæ™¯: {bg_val} | æ ¼å¼: {output_format} | å‹ç¼©: {output_compression}\n"
            f"è¯·æ±‚: {total_needed}å¼  | åˆ†æ‰¹: {len(batches)}æ¬¡ | å®é™…è·å–: {actual_returned}å¼ \n"
            f"æç¤ºè¯: {prompt[:50]}{'...' if len(prompt) > 50 else ''}"
        )
        
        return (batched, info)


# ===================================================================
#  2. GPT-Image å›¾åƒç¼–è¾‘
# ===================================================================
class GPTImageEdit:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦GPT-Image å›¾åƒç¼–è¾‘\n"
        "æ”¯æŒæœ€å¤š16å¼ å‚è€ƒå›¾ï¼Œå•å¼ è¾“å‡ºï¼ˆç¼–è¾‘APIä¸æ”¯æŒnå‚æ•°ï¼‰"
    )

    @classmethod
    def INPUT_TYPES(cls):
        # åŠ¨æ€ç”Ÿæˆ16ä¸ªå›¾åƒè¾“å…¥ç«¯å£
        optional_inputs = {
            f"reference_image_{i}": ("IMAGE",) 
            for i in range(1, 17)
        }
        
        return {
            "required": {
                "api_url": ("STRING", {"default": "https://ai.t8star.cn/v1/images/edits "}),
                "api_key": ("STRING", {"default": "", "placeholder": "sk-***"}),
                "prompt": ("STRING", {"multiline": True, "default": "ç»™äººç‰©æ·»åŠ ä¸€å‰¯å¢¨é•œï¼Œä¿æŒé£æ ¼ä¸€è‡´"}),
                "model": ("STRING", {"default": "gpt-image-1.5"}),
                "size": ([
                    "1024x1024 (æ­£æ–¹å½¢)",
                    "1536x1024 (æ¨ªç‰ˆ)",
                    "1024x1536 (ç«–ç‰ˆ)",
                    "auto (è‡ªåŠ¨)"
                ], {"default": "1024x1024 (æ­£æ–¹å½¢)"}),
            },
            "optional": {
                "quality": ([
                    "auto (è‡ªåŠ¨)",
                    "high (é«˜)",
                    "medium (ä¸­)",
                    "low (ä½)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
                "background": ([
                    "auto (è‡ªåŠ¨)",
                    "transparent (é€æ˜)",
                    "opaque (ä¸é€æ˜)"
                ], {"default": "auto (è‡ªåŠ¨)"}),
                "output_format": (["jpeg", "png", "webp"], {"default": "jpeg"}),
                "output_compression": ("INT", {"default": 90, "min": 0, "max": 100}),
                "input_fidelity": (["low (ä½ä¿çœŸ)", "high (é«˜ä¿çœŸ)"], {"default": "low (ä½ä¿çœŸ)"}),
                **optional_inputs
            }
        }

    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("image", "info")
    FUNCTION = "edit"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def parse_option(self, option_str: str) -> str:
        return option_str.split(" ")[0]

    def tensor_to_bytes(self, tensor: torch.Tensor, fmt: str = "PNG") -> bytes:
        """å¼ é‡è½¬ä¸ºå­—èŠ‚æµ"""
        pil = tensor2pil_single(tensor)
        buf = io.BytesIO()
        pil.save(buf, format=fmt)
        buf.seek(0)
        return buf.getvalue()

    def edit(self, api_url, api_key, prompt, model, size, 
             quality="auto (è‡ªåŠ¨)", background="auto (è‡ªåŠ¨)", 
             output_format="jpeg", output_compression=90, 
             input_fidelity="low (ä½ä¿çœŸ)", **kwargs):
        
        if not api_key.strip():
            print("[GPTImageEdit] âŒ API Key ç¼ºå¤±")
            return (get_empty_image(), "Error: API Key ç¼ºå¤±")

        # æ”¶é›†æ‰€æœ‰è¾“å…¥çš„å‚è€ƒå›¾ï¼ˆreference_image_1 åˆ° reference_image_16ï¼‰
        images = []
        for i in range(1, 17):
            key = f"reference_image_{i}"
            if key in kwargs and kwargs[key] is not None:
                images.append(kwargs[key])
        
        if not images:
            print("[GPTImageEdit] âŒ è‡³å°‘éœ€è¦æä¾›1å¼ å‚è€ƒå›¾")
            return (get_empty_image(), "Error: è‡³å°‘éœ€è¦1å¼ å‚è€ƒå›¾")

        print(f"[GPTImageEdit] æ”¶åˆ° {len(images)} å¼ å‚è€ƒå›¾ï¼Œå‡†å¤‡ä¸Šä¼ ...")

        size_val = self.parse_option(size)
        quality_val = self.parse_option(quality)
        bg_val = self.parse_option(background)
        fidelity_val = self.parse_option(input_fidelity)

        # æ„å»º multipart/form-dataï¼Œæ”¯æŒå¤šå›¾ä¸Šä¼ 
        files = []
        for idx, img_tensor in enumerate(images):
            img_bytes = self.tensor_to_bytes(img_tensor, "PNG")
            files.append(
                ("image", (f"input_{idx+1}.png", io.BytesIO(img_bytes), "image/png"))
            )

        data = {
            "model": model.strip(),
            "prompt": prompt,
            "size": size_val,
            "quality": quality_val,
            "background": bg_val,
            "output_format": output_format,
            "output_compression": str(output_compression),
        }
        
        # fidelity ä¸æ”¯æŒ 1-mini
        if "1-mini" not in model:
            data["input_fidelity"] = fidelity_val

        headers = {"Authorization": f"{api_key}"}

        try:
            print(f"[GPTImageEdit] å‘é€è¯·æ±‚: {model} | ä¿çœŸ: {fidelity_val} | ä¸Šä¼  {len(images)} å¼ å›¾")
            resp = requests.post(api_url, headers=headers, data=data, files=files, timeout=180)
            resp.raise_for_status()
            result = resp.json()

            # ğŸ” å…³é”®è°ƒè¯•ï¼šæ‰“å°å®Œæ•´å“åº”
            debug_str = json.dumps(result, ensure_ascii=False, indent=2)[:800]
            print(f"[GPTImageEdit] API åŸå§‹å“åº”:\n{debug_str}...")

            # æ£€æŸ¥é”™è¯¯
            if "error" in result:
                err_msg = result["error"].get("message", "æœªçŸ¥é”™è¯¯")
                print(f"[GPTImageEdit] âŒ API è¿”å›é”™è¯¯: {err_msg}")
                return (get_empty_image(), f"API Error: {err_msg}")

            if "data" not in result or not result["data"]:
                print(f"[GPTImageEdit] âš ï¸ å“åº”æ—  data å­—æ®µï¼Œå®é™…å­—æ®µ: {list(result.keys())}")
                return (get_empty_image(), "Error: å“åº”æ— å›¾åƒæ•°æ®")

            # ç¼–è¾‘APIé€šå¸¸åªè¿”å›1å¼ å›¾ï¼Œå–ç¬¬ä¸€å¼ å¤„ç†
            img_data = result["data"][0]
            
            # å°è¯•å¤šç§æ ¼å¼è§£æ
            tensor = None
            if "b64_json" in img_data and img_data["b64_json"]:
                try:
                    tensor = decode_b64_to_tensor(img_data["b64_json"])
                    print(f"[GPTImageEdit] âœ… è§£ç  base64 æˆåŠŸ")
                except Exception as e:
                    print(f"[GPTImageEdit] âš ï¸ base64 è§£ç å¤±è´¥: {e}")
            
            elif "url" in img_data and img_data["url"]:
                try:
                    url = img_data["url"]
                    print(f"[GPTImageEdit] ä¸‹è½½URL: {url[:50]}...")
                    img_resp = requests.get(url, timeout=60)
                    img_resp.raise_for_status()
                    pil_img = Image.open(io.BytesIO(img_resp.content)).convert("RGB")
                    tensor = pil2tensor(pil_img)
                    print(f"[GPTImageEdit] âœ… URL ä¸‹è½½æˆåŠŸ")
                except Exception as e:
                    print(f"[GPTImageEdit] âš ï¸ URL ä¸‹è½½å¤±è´¥: {e}")
            
            else:
                # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å­—æ®µï¼ˆå¦‚ revised_prompt ç­‰å…ƒæ•°æ®ï¼‰
                print(f"[GPTImageEdit] âš ï¸ æ—  b64_json æˆ– urlï¼Œå¯ç”¨å­—æ®µ: {list(img_data.keys())}")
                return (get_empty_image(), f"Error: æ— æ³•è§£æå›¾åƒï¼Œå­—æ®µ: {list(img_data.keys())}")

            if tensor is None:
                return (get_empty_image(), "Error: å›¾åƒè§£ç å¤±è´¥")

            if tensor.dim() == 3:
                tensor = tensor.unsqueeze(0)

            info = (
                f"âœï¸ GPT-Image Edit | {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"æ¨¡å‹: {model} | å°ºå¯¸: {size_val} | ä¿çœŸ: {fidelity_val}\n"
                f"ä¸Šä¼ : {len(images)}å¼ å‚è€ƒå›¾ | è¾“å‡ºæ ¼å¼: {output_format}\n"
                f"æç¤ºè¯: {prompt[:50]}{'...' if len(prompt) > 50 else ''}"
            )
            
            print(f"[GPTImageEdit] âœ… ç¼–è¾‘æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {tensor.shape}")
            return (tensor, info)

        except requests.exceptions.HTTPError as e:
            err_text = e.response.text if e.response else str(e)
            print(f"[GPTImageEdit] âŒ HTTP é”™è¯¯: {err_text[:200]}")
            return (get_empty_image(), f"HTTP Error: {err_text[:200]}")
        except Exception as e:
            err_msg = f"[GPTImageEdit] âŒ è¯·æ±‚å¤±è´¥: {str(e)}"
            print(err_msg)
            return (get_empty_image(), err_msg)


# ===================================================================
#  3. GPT-Image ç¼–è¾‘æäº¤èŠ‚ç‚¹
# ===================================================================
class GPTImageEditSubmit:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦GPT-Image ç¼–è¾‘æäº¤ | çœŸå¹¶å‘\n"
        "å¯åŠ¨åå°çº¿ç¨‹ï¼Œç«‹å³è¿”å› task_idï¼Œä¸é˜»å¡å·¥ä½œæµ"
    )

    @classmethod
    def INPUT_TYPES(cls):
        optional_inputs = {f"reference_image_{i}": ("IMAGE",) for i in range(1, 17)}
        return {
            "required": {
                "api_url": ("STRING", {"default": "https://ai.t8star.cn/v1/images/edits "}),
                "api_key": ("STRING", {"default": "", "placeholder": "sk-***"}),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "model": ("STRING", {"default": "gpt-image-1.5"}),
                "size": (["1024x1024 (æ­£æ–¹å½¢)", "1536x1024 (æ¨ªç‰ˆ)", "1024x1536 (ç«–ç‰ˆ)", "auto (è‡ªåŠ¨)"], {"default": "1024x1024 (æ­£æ–¹å½¢)"}),
            },
            "optional": {
                "quality": (["auto (è‡ªåŠ¨)", "high (é«˜)", "medium (ä¸­)", "low (ä½)"], {"default": "auto (è‡ªåŠ¨)"}),
                "background": (["auto (è‡ªåŠ¨)", "transparent (é€æ˜)", "opaque (ä¸é€æ˜)"], {"default": "auto (è‡ªåŠ¨)"}),
                "output_format": (["jpeg", "png", "webp"], {"default": "jpeg"}),
                "output_compression": ("INT", {"default": 90, "min": 0, "max": 100}),
                "input_fidelity": (["low (ä½ä¿çœŸ)", "high (é«˜ä¿çœŸ)"], {"default": "low (ä½ä¿çœŸ)"}),
                **optional_inputs
            }
        }

    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("task_id", "status")
    FUNCTION = "submit"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def parse_option(self, s): 
        return s.split(" ")[0]

    def submit(self, api_url, api_key, prompt, model, size, **kwargs):
        """
        ç”Ÿæˆ task_idï¼Œå¯åŠ¨åå°çº¿ç¨‹ï¼Œç«‹å³è¿”å›
        å®ç°çœŸæ­£çš„å¹¶å‘ï¼šå¤šä¸ª Submit èŠ‚ç‚¹å¯åŒæ—¶å¯åŠ¨
        """
        if not api_key.strip():
            return ("", "Error: API Key missing")

        # æ”¶é›†å›¾ç‰‡
        images = [kwargs.get(f"reference_image_{i}") for i in range(1, 17) 
                  if kwargs.get(f"reference_image_{i}") is not None]
        
        if not images:
            return ("", "Error: No input images")

        # ç”Ÿæˆå”¯ä¸€ ID å’ŒåŒæ­¥äº‹ä»¶
        task_id = str(uuid.uuid4())
        event = threading.Event()
        
        with _cache_lock:
            _processing_events[task_id] = event

        # å‡†å¤‡å‚æ•°
        size_val = self.parse_option(size)
        quality_val = self.parse_option(kwargs.get("quality", "auto (è‡ªåŠ¨)"))
        bg_val = self.parse_option(kwargs.get("background", "auto (è‡ªåŠ¨)"))
        fidelity_val = self.parse_option(kwargs.get("input_fidelity", "low (ä½ä¿çœŸ)"))
        out_format = kwargs.get("output_format", "jpeg")
        out_compress = kwargs.get("output_compression", 90)

        # åå°ä»»åŠ¡å‡½æ•°
        def worker():
            try:
                files = []
                for idx, img in enumerate(images):
                    pil = tensor2pil_single(img)
                    buf = io.BytesIO()
                    pil.save(buf, format="PNG")
                    buf.seek(0)
                    files.append(("image", (f"ref_{idx}.png", buf, "image/png")))

                data = {
                    "model": model.strip(),
                    "prompt": prompt,
                    "size": size_val,
                    "quality": quality_val,
                    "background": bg_val,
                    "output_format": out_format,
                    "output_compression": str(out_compress),
                }
                if "1-mini" not in model:
                    data["input_fidelity"] = fidelity_val

                print(f"[GPTImageEditSubmit] åå°å¼€å§‹ | task: {task_id[:8]} | å›¾ç‰‡: {len(images)}å¼ ")
                
                resp = requests.post(api_url, headers={"Authorization": api_key}, 
                                   data=data, files=files, timeout=180)
                resp.raise_for_status()
                result = resp.json()

                tensor = None
                if "data" in result and result["data"]:
                    img_data = result["data"][0]
                    if "b64_json" in img_data and img_data["b64_json"]:
                        tensor = decode_b64_to_tensor(img_data["b64_json"])
                        if tensor.dim() == 3:
                            tensor = tensor.unsqueeze(0)
                    elif "url" in img_data and img_data["url"]:
                        r = requests.get(img_data["url"], timeout=60)
                        pil = Image.open(io.BytesIO(r.content)).convert("RGB")
                        tensor = pil2tensor(pil)
                
                if tensor is not None:
                    cache_result(task_id, tensor)
                    print(f"[GPTImageEditSubmit] åå°å®Œæˆ | task: {task_id[:8]} | æˆåŠŸ")
                else:
                    cache_result(task_id, None)
                    print(f"[GPTImageEditSubmit] åå°å®Œæˆ | task: {task_id[:8]} | æ— å›¾åƒ")
                    
            except Exception as e:
                print(f"[GPTImageEditSubmit] åå°å¼‚å¸¸ | task: {task_id[:8]} | {e}")
                cache_result(task_id, None)

        # å¯åŠ¨åå°çº¿ç¨‹ï¼Œç«‹å³è¿”å›
        thread = threading.Thread(target=worker, daemon=True)
        thread.start()
        
        print(f"[GPTImageEditSubmit] å·²æäº¤ | task_id: {task_id[:8]}...")
        return (task_id, "Submitted")


# ===================================================================
#  4. æ”¶é›†èŠ‚ç‚¹
# ===================================================================
class GPTImageEditCollect:
    DESCRIPTION = (
        "ğŸ’• å“å‘€âœ¦ä»»åŠ¡æ”¶é›†å™¨ | ç»Ÿä¸€ç­‰å¾…\n"
        "é˜»å¡ç­‰å¾…9ä¸ªä»»åŠ¡å…¨éƒ¨å®Œæˆï¼Œå¤±è´¥å¡«ç©ºç™½å›¾"
    )

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "optional": {
                "task_id_1": ("STRING", {"forceInput": True}),
                "task_id_2": ("STRING", {"forceInput": True}),
                "task_id_3": ("STRING", {"forceInput": True}),
                "task_id_4": ("STRING", {"forceInput": True}),
                "task_id_5": ("STRING", {"forceInput": True}),
                "task_id_6": ("STRING", {"forceInput": True}),
                "task_id_7": ("STRING", {"forceInput": True}),
                "task_id_8": ("STRING", {"forceInput": True}),
                "task_id_9": ("STRING", {"forceInput": True}),
            }
        }

    RETURN_TYPES = ("IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE", "IMAGE")
    RETURN_NAMES = ("image_1", "image_2", "image_3", "image_4", "image_5", "image_6", "image_7", "image_8", "image_9")
    FUNCTION = "collect"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"

    def collect(self, task_id_1=None, task_id_2=None, task_id_3=None, 
                task_id_4=None, task_id_5=None, task_id_6=None,
                task_id_7=None, task_id_8=None, task_id_9=None):
        """
        ç»Ÿä¸€ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆæœ€å¤šç­‰300ç§’ï¼‰
        å¦‚æœæŸé€šé“æœªè¿æ¥ï¼Œä¹Ÿè¿”å›ç©ºå›¾å ä½
        """
        task_ids = [task_id_1, task_id_2, task_id_3, task_id_4, task_id_5, 
                   task_id_6, task_id_7, task_id_8, task_id_9]
        
        print(f"[Collect] å¼€å§‹æ”¶é›†ï¼Œæ£€æŸ¥9ä¸ªé€šé“...")
        
        # å…ˆç»Ÿè®¡æœ‰æ•ˆçš„ä»»åŠ¡
        valid_tasks = [(i, tid) for i, tid in enumerate(task_ids, 1) if tid]
        if not valid_tasks:
            print("[Collect] æ— æœ‰æ•ˆä»»åŠ¡ï¼Œå…¨éƒ¨è¿”å›ç©ºå›¾")
            return tuple([get_empty_image() for _ in range(9)])
        
        print(f"[Collect] æœ‰æ•ˆä»»åŠ¡: {len(valid_tasks)}ä¸ªï¼Œå¼€å§‹ç­‰å¾…...")
        
        # ç»Ÿä¸€ç­‰å¾…æ‰€æœ‰æœ‰æ•ˆä»»åŠ¡ï¼ˆæœ€å¤š300ç§’ï¼‰
        max_wait = 300  # 5åˆ†é’Ÿè¶…æ—¶
        start_time = time.time()
        all_done = False
        
        while not all_done and (time.time() - start_time) < max_wait:
            all_done = True
            for idx, tid in valid_tasks:
                if get_result(tid) is None:
                    # è¿˜åœ¨å¤„ç†ä¸­
                    all_done = False
                    break
            
            if not all_done:
                time.sleep(0.5)  # è½®è¯¢é—´éš”
        
        # æ”¶é›†ç»“æœ
        results = []
        for i, tid in enumerate(task_ids, 1):
            if not tid:
                results.append(get_empty_image())
                print(f"[Collect] é€šé“{i}: æœªè¿æ¥")
            else:
                tensor = get_result(tid)
                if tensor is not None:
                    results.append(tensor)
                    print(f"[Collect] é€šé“{i}: æˆåŠŸ ({tid[:8]})")
                else:
                    # å¤±è´¥æˆ–è¶…æ—¶ï¼Œæ ¹æ®è¾“å…¥å›¾æ¨æ–­å°ºå¯¸ï¼Ÿè¿™é‡Œç»Ÿä¸€ç”¨1024x1024
                    results.append(get_empty_image())
                    print(f"[Collect] é€šé“{i}: å¤±è´¥/è¶…æ—¶ ({tid[:8]})")
        
        print(f"[Collect] æ”¶é›†å®Œæˆï¼Œè¾“å‡º9å¼ å›¾")
        return tuple(results)


# ===================================================================
#  5. SVG ç”ŸæˆèŠ‚ç‚¹
# ===================================================================
import re
from pathlib import Path
from PIL import ImageDraw, ImageFont
import numpy as np

def svg_extract_from_text(text: str) -> str:
    """ä»APIè¿”å›æ–‡æœ¬ä¸­æå–SVGä»£ç """
    if not text:
        return ""
    cleaned = re.sub(r'```svg\s*', '', text, flags=re.IGNORECASE)
    cleaned = re.sub(r'```\s*$', '', cleaned)
    cleaned = re.sub(r'```', '', cleaned).strip()
    
    svg_match = re.search(r'(<svg[^>]*>)', cleaned, re.IGNORECASE | re.DOTALL)
    if svg_match:
        start_idx = svg_match.start()
        if start_idx > 5 and cleaned[start_idx-5:start_idx].find('?>') > -1:
            xml_start = cleaned.rfind('<?xml', 0, start_idx)
            if xml_start != -1:
                start_idx = xml_start
        cleaned = cleaned[start_idx:]
    
    end_match = re.search(r'</svg\s*>', cleaned, re.IGNORECASE)
    if end_match:
        cleaned = cleaned[:end_match.end()]
    
    if '<svg' in cleaned.lower() and '</svg>' in cleaned.lower():
        return cleaned.strip()
    return ""

def svg_has_animation(svg_code: str) -> bool:
    """æ£€æµ‹SVGæ˜¯å¦åŒ…å«åŠ¨ç”»"""
    indicators = ['<animate', '<animateTransform', 'animation:', '@keyframes', 'transition:']
    return any(ind in svg_code.lower() for ind in indicators)

def svg_to_tensor(svg_code: str, width: int = 512, height: int = 512):
    """
    SVGè½¬PNGé¢„è§ˆå›¾ï¼ˆå¼ºåˆ¶å½©è‰²è¾“å‡ºï¼‰
    ä¾èµ–ï¼špip install pymupdf (fitz)
    """
    if not svg_code or not svg_code.strip():
        img = np.ones((height, width, 3), dtype=np.float32) * 0.95
        return torch.from_numpy(img)
    
    # æ–¹æ¡ˆ1: PyMuPDFï¼ˆå¼ºåˆ¶RGBå½©è‰²ï¼‰
    try:
        import fitz
        doc = fitz.open(stream=svg_code.encode('utf-8'), filetype="svg")
        page = doc[0]
        rect = page.rect
        
        if rect.width > 0 and rect.height > 0:
            zoom = min(width / rect.width, height / rect.height)
            mat = fitz.Matrix(zoom, zoom)
        else:
            mat = fitz.Matrix(1, 1)
        
        pix = page.get_pixmap(matrix=mat, colorspace=fitz.csRGB, alpha=False)
        
        if pix.n == 3:
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        elif pix.n == 4:
            img = Image.frombytes("RGBA", [pix.width, pix.height], pix.samples).convert("RGB")
        else:
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.tobytes("png"))
        
        doc.close()
        
        if img.width != width or img.height != height:
            img = img.resize((width, height), Image.Resampling.LANCZOS)
        
        return pil2tensor(img)
    except Exception as e:
        print(f"[SVG] PyMuPDFæ¸²æŸ“å¤±è´¥(å®‰è£…pymupdfè·å¾—æœ€ä½³æ•ˆæœ): {e}")
    
    # æ–¹æ¡ˆ2: cairosvgï¼ˆå¤‡ç”¨ï¼‰
    try:
        import cairosvg
        png_data = cairosvg.svg2png(
            bytestring=svg_code.encode('utf-8'),
            output_width=width,
            output_height=height,
            background_color='white'
        )
        return pil2tensor(Image.open(io.BytesIO(png_data)).convert('RGB'))
    except:
        pass
    
    # æ–¹æ¡ˆ3: å½©è‰²ä»£ç æˆªå›¾ï¼ˆé™çº§ï¼‰
    pil_img = Image.new('RGB', (width, height), color=(30, 30, 30))
    draw = ImageDraw.Draw(pil_img)
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSansMono.ttf", 10)
        header_font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 12)
    except:
        font = ImageFont.load_default()
        header_font = font
    
    draw.rectangle([0, 0, width, 26], fill=(40, 90, 140))
    draw.text((8, 5), "SVG Preview (Install PyMuPDF)", fill=(255, 255, 255), font=header_font)
    
    lines = svg_code.split('\n')[:40]
    y_pos = 32
    for i, line in enumerate(lines):
        if y_pos > height - 12:
            break
        color = (200, 200, 200)
        if line.strip().startswith('<'):
            color = (100, 180, 255)
        elif '=' in line:
            color = (255, 180, 80)
        draw.text((8, y_pos), line[:100], fill=color, font=font)
        y_pos += 12
    
    return pil2tensor(pil_img)

def tensor_to_base64(tensor: torch.Tensor) -> str:
    """å¼ é‡è½¬base64 PNGï¼ˆç”¨äºvisionè¾“å…¥ï¼‰"""
    if tensor.dim() == 4:
        tensor = tensor[0]
    tensor = (tensor.clamp(0, 1) * 255).byte().cpu()
    buf = io.BytesIO()
    Image.fromarray(tensor.numpy()).save(buf, format="PNG")
    return base64.b64encode(buf.getvalue()).decode()

def svg_save_html(svg_code: str, output_dir: Path, prefix: str) -> str:
    """ä¿å­˜SVGä¸ºHTMLä»¥é¢„è§ˆåŠ¨ç”»"""
    try:
        import folder_paths
        if not output_dir.exists():
            output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = int(time.time())
        html_path = output_dir / f"{prefix}_{timestamp}.html"
        is_anim = svg_has_animation(svg_code)
        
        html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>SVG {'(Animated)' if is_anim else ''}</title>
    <style>
        body {{ margin: 0; padding: 20px; background: #f0f0f0; display: flex; justify-content: center; align-items: center; min-height: 100vh; font-family: Arial; }}
        .container {{ background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        .info {{ color: #666; font-size: 12px; margin-bottom: 10px; }}
        svg {{ border: 1px solid #ddd; max-width: 100%; }}
        {'svg * {{ animation-play-state: running !important; }}' if is_anim else ''}
    </style>
</head>
<body>
    <div class="container">
        <div class="info">{time.strftime('%Y-%m-%d %H:%M:%S')} | {len(svg_code)} chars | {'Animated' if is_anim else 'Static'}</div>
        {svg_code}
    </div>
</body>
</html>"""
        
        with open(html_path, 'w', encoding='utf-8') as f:
            f.write(html)
        return str(html_path.absolute())
    except Exception as e:
        print(f"[SVG HTML Save Error] {e}")
        return ""

class SVG_Generate_Save_mmx:
    DESCRIPTION = (
        "å›¾å‚è€ƒï¼Œç”ŸæˆSVG"
    )

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "placeholder": "sk-***"}),
                "api_url": ("STRING", {"default": "https://ai.t8star.cn/v1/chat/completions"}),
                "prompt": ("STRING", {"multiline": True, "default": "ç”Ÿæˆä¸€ä¸ªæç®€ç§‘æŠ€åŠ¨ç”»ï¼Œè“è‰²æ¸å˜"}),
                "model": ("STRING", {"default": "gemini-3-flash-preview"}),
                "filename_prefix": ("STRING", {"default": "svg_output"}),
                "width": ("INT", {"default": 512, "min": 64, "max": 2048, "step": 64}),
                "height": ("INT", {"default": 512, "min": 64, "max": 2048, "step": 64}),
            },
            "optional": {
                "temperature": ("FLOAT", {"default": 0.4, "min": 0.0, "max": 2.0, "step": 0.1}),
                "max_tokens": ("INT", {"default": 4096, "min": 1024, "max": 8192000, "step": 1024}),
                "save_html": ("BOOLEAN", {"default": True, "tooltip": "å«åŠ¨ç”»æ—¶ä¿å­˜HTML"}),
                **{f"image_{i}": ("IMAGE",) for i in range(1, 15)}
            }
        }

    RETURN_TYPES = ("STRING", "IMAGE", "STRING", "INT", "INT", "STRING", "STRING")
    RETURN_NAMES = ("filepath", "preview_image", "html_path", "width", "height", "svg_code", "info")
    FUNCTION = "generate_and_save"
    CATEGORY = "å“å‘€âœ¦MMX/å›¾åƒ"
    OUTPUT_NODE = True

    def build_messages(self, prompt: str, images: list, width: int, height: int):
        """æ„å»ºOpenAIæ ¼å¼æ¶ˆæ¯"""
        system_prompt = f"""You are an expert SVG designer.
Generate standard SVG code based on user description.
Viewport: viewBox="0 0 {width} {height}"
Requirements:
1. Valid SVG with xmlns="http://www.w3.org/2000/svg"
2. Use vector elements (path, circle, rect), no base64 images
3. Professional colors, hex values
4. Return ONLY SVG code, no markdown, no explanations
5. Support gradients and animations if requested"""

        messages = [{"role": "system", "content": system_prompt}]
        content = []
        
        # æ·»åŠ å‚è€ƒå›¾ï¼ˆvisionæ ¼å¼ï¼‰
        for img_tensor in images:
            if img_tensor is not None:
                b64 = tensor_to_base64(img_tensor)
                content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{b64}"}
                })
        
        content.append({
            "type": "text", 
            "text": f"Generate SVG: {prompt}\n\nSVG Code:"
        })
        
        messages.append({"role": "user", "content": content})
        return messages

    def generate_and_save(self, api_key, api_url, prompt, model, filename_prefix, 
                         width, height, temperature=0.4, max_tokens=4096, 
                         save_html=True, **image_inputs):
        
        if not api_key.strip():
            empty = torch.zeros(1, height, width, 3)
            return ("", empty, "", width, height, "", "Error: API Key missing")

        # æ”¶é›†å›¾åƒ
        images = [image_inputs.get(f"image_{i}") for i in range(1, 15)]
        valid_images = [img for img in images if img is not None]
        
        try:
            print(f"[SVG] å¼€å§‹ç”Ÿæˆ | æ¨¡å‹: {model} | å°ºå¯¸: {width}x{height} | å‚è€ƒå›¾: {len(valid_images)}å¼ ")
            
            # 1. è°ƒç”¨APIç”Ÿæˆ
            messages = self.build_messages(prompt, valid_images, width, height)
            payload = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "top_p": 0.95,
            }
            headers = {
                "Authorization": f"Bearer {api_key.strip()}",
                "Content-Type": "application/json"
            }
            
            resp = requests.post(api_url.strip(), headers=headers, json=payload, timeout=300)
            resp.raise_for_status()
            data = resp.json()
            raw_content = data["choices"][0]["message"]["content"]
            
            # 2. æå–SVG
            svg_code = svg_extract_from_text(raw_content)
            if not svg_code:
                print(f"[SVG] æå–SVGå¤±è´¥: {raw_content[:300]}")
                return ("", torch.zeros(1, height, width, 3), "", width, height, "", "Error: No valid SVG")
            
            # 3. ç”Ÿæˆå½©è‰²é¢„è§ˆå›¾ï¼ˆPyMuPDFä¼˜å…ˆï¼‰
            preview = svg_to_tensor(svg_code, width, height)
            
            # 4. ä¿å­˜SVGæ–‡ä»¶
            import folder_paths
            output_dir = Path(folder_paths.get_output_directory())
            svg_dir = output_dir / "svg"
            svg_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = int(time.time())
            filename = f"{filename_prefix}_{timestamp}.svg"
            filepath = svg_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(svg_code.strip())
            
            abs_path = str(filepath.absolute())
            
            # 5. ä¿å­˜HTMLï¼ˆå¦‚æœå«åŠ¨ç”»ï¼‰
            html_path = ""
            if save_html and svg_has_animation(svg_code):
                html_dir = output_dir / "svg_html"
                html_path = svg_save_html(svg_code, html_dir, filename_prefix)
            
            is_anim = svg_has_animation(svg_code)
            info = f"âœ… æˆåŠŸ | æ¨¡å‹: {model} | ä»£ç : {len(svg_code)}å­—ç¬¦ | åŠ¨ç”»: {'æ˜¯' if is_anim else 'å¦'}"
            print(f"[SVG] å®Œæˆ | å·²ä¿å­˜: {filename} ({len(svg_code)}å­—ç¬¦)")
            
            return (abs_path, preview, html_path, width, height, svg_code, info)
            
        except Exception as e:
            print(f"[SVG] ç”Ÿæˆå¤±è´¥: {e}")
            return ("", torch.zeros(1, height, width, 3), "", width, height, "", f"Error: {e}")


# ===================================================================
#  ç»Ÿä¸€æ³¨å†Œ
# ===================================================================
register_node(GPTImageGenerate, "GPTImage_Generate_mmx")
register_node(GPTImageEdit, "GPTImage_Edit_mmx")
register_node(GPTImageEditSubmit, "GPTImage_Edit_Submit_mmx")
register_node(GPTImageEditCollect, "GPTImage_Edit_Collect_mmx")
register_node(SVG_Generate_Save_mmx, "SVG_Generate_mmx")

