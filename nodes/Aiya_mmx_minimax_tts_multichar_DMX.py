# Aiya_mmx_minimax_tts_multichar_DMX.py
from __future__ import annotations
import io
import os
import re
import json
import requests
import torch
import soundfile as sf
from datetime import datetime
import folder_paths
from ..register import register_node
# å¤ç”¨å•éŸ³è‰²èŠ‚ç‚¹çš„ generate_speech é€»è¾‘ä¸éŸ³è‰²åˆ—è¡¨
from .Aiya_mmx_minimax_tts_DMX import MiniMaxTTS_DMX, VOICE_PRESETS


class MiniMaxTTSMultiChar_DMX:
    DESCRIPTION = (
        "ğŸ’• MiniMax å¤šäººå¯¹è¯ TTSï¼ˆspeech-2.6-hdï¼‰\n\n"
        "ã€ç”¨æ³•ã€‘\n"
        "1) script ç«¯å£æ¯è¡Œæ ¼å¼ï¼š\n"
        "     è§’è‰²|è¯­é€Ÿ|éŸ³é«˜|æƒ…ç»ª:æ–‡æœ¬   ï¼ˆåä¸‰é¡¹å¯çœç•¥ï¼Œé»˜è®¤ 1.0/0/neutralï¼‰\n"
        "   ä¾‹ï¼š\n"
        "     å°æ˜|1.2:ä»Šå¤©æˆ‘ä»¬å»åƒç«é”…å§ï¼\n"
        "     å°çº¢|0.9|+2|happy:è¶…å¼€å¿ƒï¼\n"
        "     å°åˆš:æˆ‘å°±ç”¨é»˜è®¤å‚æ•°\n"
        "2) voice_map ç«¯å£å†™ã€Œè§’è‰²=éŸ³è‰²IDã€æ˜ å°„ï¼Œä¸€è¡Œä¸€æ¡ã€‚\n"
        "3) å…¶ä½™å‚æ•°ï¼ˆé‡‡æ ·ç‡ã€æ ¼å¼ç­‰ï¼‰å…¨å±€é»˜è®¤ï¼›å•ç‹¬å†™çš„ä¼˜å…ˆçº§>å…¨å±€ã€‚\n"
        "4) è¾“å‡ºä¸€æ¡æ‹¼æ¥å¥½çš„é•¿éŸ³é¢‘ + æ¯å¥ infoï¼ˆæ¢è¡Œåˆ†éš”ï¼‰ã€‚\n"
        "5) ä»»æ„å¥å­åˆæˆå¤±è´¥è‡ªåŠ¨æ’å…¥ 0.1 s é™éŸ³ï¼Œä¸‹æ¸¸æ°¸ä¸å´©æºƒã€‚\n"
    )

    RETURN_TYPES = ("AUDIO", "STRING")
    RETURN_NAMES = ("æ‹¼æ¥éŸ³é¢‘", "info")
    FUNCTION = "generate_multichar_speech"
    CATEGORY = "å“å‘€âœ¦MMX/DMXAPI"
    OUTPUT_NODE = True

    def __init__(self):
        self.worker = MiniMaxTTS_DMX()

    # ---------------- å°å·¥å…· ----------------
    @staticmethod
    def _make_silence_tensor(sec: float, sr: int):
        n = int(sec * sr)
        return torch.zeros(1, 1, n)

    @staticmethod
    def _parse_script(script: str):
        """
        è§£æå‰§æœ¬
        æ¯è¡Œæ ¼å¼ï¼š  è§’è‰²|speed|pitch|emotion:æ–‡æœ¬
        è¿”å› List[Dict{'role','speed','pitch','emotion','text'}]
        ç¼ºçœå€¼ï¼šspeed=1.0  pitch=0  emotion='neutral'
        """
        lines = [ln.strip() for ln in script.splitlines() if ln.strip()]
        out = []
        for ln in lines:
            if ':' not in ln:
                continue
            head, txt = ln.split(':', 1)
            # é»˜è®¤å€¼
            role, speed, pitch, emotion = head.strip(), 1.0, 0, 'neutral'
            # æŒ‰ | æ‹†åˆ†æœ€å¤š 4 æ®µ
            parts = [p.strip() for p in head.split('|')]
            if len(parts) >= 1:
                role = parts[0]
            if len(parts) >= 2:
                try:
                    speed = float(parts[1])
                except ValueError:
                    speed = 1.0
            if len(parts) >= 3:
                try:
                    pitch = int(parts[2])
                except ValueError:
                    pitch = 0
            if len(parts) >= 4:
                emotion = parts[3] if parts[3] in {"neutral", "happy", "sad", "angry", "fearful", "surprised"} else "neutral"
            out.append({"role": role, "speed": speed, "pitch": pitch, "emotion": emotion, "text": txt.strip()})
        return out

    @staticmethod
    def _parse_voice_map(voice_map: str):
        mp = {}
        for ln in voice_map.splitlines():
            ln = ln.strip()
            if not ln or '=' not in ln:
                continue
            role, vid = ln.split('=', 1)
            mp[role.strip()] = vid.strip()
        return mp

    # ---------------- è¾“å…¥ç«¯å£ ----------------
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "api_key": ("STRING", {
                    "default": "", "placeholder": "sk-***************************"
                }),
                "script": ("STRING", {
                    "multiline": True,
                    "default": "å°æ˜|1.2:ä»Šå¤©æˆ‘ä»¬å»åƒç«é”…å§ï¼\nå°çº¢|0.9:è¶…å¼€å¿ƒï¼\nå°åˆš:æˆ‘å°±ç”¨é»˜è®¤å‚æ•°",
                    "placeholder": "è§’è‰²|speed|pitch|emotion:æ–‡æœ¬  ï¼ˆåä¸‰é¡¹å¯çœç•¥ï¼‰"
                }),
                "voice_map": ("STRING", {
                    "multiline": True,
                    "default": "å°æ˜=male-qn-qingse\nå°çº¢=female-tianmei\nå°åˆš=male-qn-jingying",
                    "placeholder": "è§’è‰²=éŸ³è‰²ID  ä¸€è¡Œä¸€æ¡"
                }),
                "model": (["speech-2.6-hd"], {"default": "speech-2.6-hd"}),
                "speed": ("FLOAT", {
                    "default": 1.0, "min": 0.5, "max": 2.0, "step": 0.05, "display": "slider"
                }),
                "pitch": ("INT", {
                    "default": 0, "min": -12, "max": 12, "step": 1, "display": "slider"
                }),
                "volume": ("FLOAT", {
                    "default": 1.0, "min": 0.0, "max": 10.0, "step": 0.1, "display": "slider"
                }),
                "emotion": (["neutral", "happy", "sad", "angry", "fearful", "surprised"], {"default": "neutral"}),
                "audio_format": (["mp3", "wav"], {"default": "mp3"}),
                "sample_rate": ("INT", {
                    "default": 24000, "min": 16000, "max": 48000, "step": 8000
                }),
            }
        }

    # ---------------- ä¸»å…¥å£ ----------------
    def generate_multichar_speech(
        self,
        api_key,
        script,
        voice_map,
        model,
        speed,
        pitch,
        volume,
        emotion,
        audio_format,
        sample_rate,
    ):
        if not api_key.strip():
            return ({"waveform": torch.zeros(1, 1, 1), "sample_rate": 24000}, "âŒ API Key ä¸ºç©º")
        dialogue = self._parse_script(script)
        role2voice = self._parse_voice_map(voice_map)
        if not dialogue:
            return ({"waveform": torch.zeros(1, 1, 1), "sample_rate": 24000}, "âŒ å‰§æœ¬è§£æä¸ºç©º")
        if not role2voice:
            return ({"waveform": torch.zeros(1, 1, 1), "sample_rate": 24000}, "âŒ éŸ³è‰²æ˜ å°„ä¸ºç©º")

        wav_list, info_list = [], []
        for idx, item in enumerate(dialogue, 1):
            role, text = item["role"], item["text"]
            # æœ¬å¥å‚æ•°ä¼˜å…ˆç”¨å‰§æœ¬é‡Œå†™çš„ï¼Œæ²¡å†™å†å›è½åˆ°å…¨å±€
            spd = item.get("speed", speed)
            ptc = item.get("pitch", pitch)
            emo = item.get("emotion", emotion)
            voice_id = role2voice.get(role)
            if not voice_id:
                err = f"ç¬¬{idx}å¥è§’è‰²ã€{role}ã€æœªåœ¨ voice_map ä¸­æ‰¾åˆ°æ˜ å°„ï¼Œå·²æ’å…¥é™éŸ³"
                info_list.append(err)
                wav_list.append(self._make_silence_tensor(0.1, sample_rate))
                continue

            # è°ƒç”¨å•éŸ³è‰² worker
            audio_dict, info = self.worker.generate_speech(
                api_key=api_key,
                text=text,
                model=model,
                voice_id=voice_id,
                speed=spd,
                pitch=ptc,
                volume=volume,
                emotion=emo,
                audio_format=audio_format,
                sample_rate=sample_rate,
            )
            if "âŒ" in info:
                wav_list.append(self._make_silence_tensor(0.1, sample_rate))
                info_list.append(f"ç¬¬{idx}å¥({role}) å¤±è´¥: {info}")
            else:
                wav_list.append(audio_dict["waveform"])
                info_list.append(f"#{idx}({role}|spd={spd}|ptc={ptc}|emo={emo}) {info}")

        # æ‹¼æ¥
        full_wave = torch.cat(wav_list, dim=-1)
        final_audio = {"waveform": full_wave, "sample_rate": sample_rate}
        final_info = "\n".join(info_list)
        return (final_audio, final_info)


register_node(MiniMaxTTSMultiChar_DMX, "MiniMax TTS å¤šäººå¯¹è¯_DMX")
